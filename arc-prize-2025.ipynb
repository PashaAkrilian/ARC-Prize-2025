{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:08:10.444742Z","iopub.execute_input":"2025-07-17T19:08:10.444919Z","iopub.status.idle":"2025-07-17T19:08:13.115364Z","shell.execute_reply.started":"2025-07-17T19:08:10.444903Z","shell.execute_reply":"2025-07-17T19:08:13.114430Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/arc-prize-2025/arc-agi_training_solutions.json\n/kaggle/input/arc-prize-2025/arc-agi_evaluation_solutions.json\n/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json\n/kaggle/input/arc-prize-2025/sample_submission.json\n/kaggle/input/arc-prize-2025/arc-agi_training_challenges.json\n/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"**Setup & Instalasi**","metadata":{}},{"cell_type":"code","source":"!pip install networkx tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:08:17.307603Z","iopub.execute_input":"2025-07-17T19:08:17.308249Z","iopub.status.idle":"2025-07-17T19:08:22.653846Z","shell.execute_reply.started":"2025-07-17T19:08:17.308225Z","shell.execute_reply":"2025-07-17T19:08:22.653072Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 1) Import library dasar\nimport os\nimport json\nimport numpy as np\nimport torch\nimport random\n\n# 2) Pastikan reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# 3) Cek perangkat (CPU/GPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# 4) Path data\nDATA_DIR = '/kaggle/input/arc-prize-2025'\n\n# 5) (Opsional) Install dependensi tambahan\n#    Kaggle image container sudah menyertakan PyTorch, NumPy, dll.\n#    Jika butuh library lain, uncomment dan sesuaikan:\n\n\n# 6) Buat direktori kerja untuk output/model\nWORK_DIR = '/kaggle/working'\nos.makedirs(WORK_DIR, exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:08:25.176805Z","iopub.execute_input":"2025-07-17T19:08:25.177574Z","iopub.status.idle":"2025-07-17T19:08:31.810543Z","shell.execute_reply.started":"2025-07-17T19:08:25.177530Z","shell.execute_reply":"2025-07-17T19:08:31.809901Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Eksplorasi Data (EDA) Singkat","metadata":{}},{"cell_type":"code","source":"import os, json, numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport random\n\n# 1) Load mentah, lalu adaptif ke list of tasks\npath = '/kaggle/input/arc-prize-2025/arc-agi_training_challenges.json'\nwith open(path) as f:\n    raw = json.load(f)\n\n# Jika top-level dict, ambil child yang berisi list tasks\nif isinstance(raw, dict):\n    # Cek apakah ada kunci 'tasks'\n    if 'tasks' in raw:\n        train_tasks = raw['tasks']\n    else:\n        # Atau mungkin mapping id → task\n        train_tasks = list(raw.values())\nelse:\n    train_tasks = raw\n\nprint(f\"Loaded {len(train_tasks)} tasks\")\n\n# 2) EDA: kumpulkan statistik\nrecords = []\nfor task in train_tasks:\n    # pastikan setiap task juga dict\n    assert isinstance(task, dict), task\n    for ex in task['train']:\n        inp = np.array(ex['input'])\n        out = np.array(ex['output'])\n        for role, grid in [('input', inp), ('output', out)]:\n            records.append({\n                'role': role,\n                'h': grid.shape[0],\n                'w': grid.shape[1],\n                'n_colors': int(np.unique(grid).size)\n            })\n\ndf = pd.DataFrame(records)\nsummary = df.groupby('role').agg({\n    'h': ['min','max','mean'],\n    'w': ['min','max','mean'],\n    'n_colors': ['min','max','mean']\n}).round(2)\nprint(summary)\n\n# 3) Visualisasi contoh acak\nsample = random.choice(train_tasks)\ninp0, out0 = map(np.array, (sample['train'][0]['input'], sample['train'][0]['output']))\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(6,3))\nax1.imshow(inp0, interpolation='nearest'); ax1.set_title('Input'); ax1.axis('off')\nax2.imshow(out0, interpolation='nearest'); ax2.set_title('Output'); ax2.axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:08:35.332184Z","iopub.execute_input":"2025-07-17T19:08:35.332997Z","iopub.status.idle":"2025-07-17T19:08:36.187083Z","shell.execute_reply.started":"2025-07-17T19:08:35.332968Z","shell.execute_reply":"2025-07-17T19:08:36.186316Z"}},"outputs":[{"name":"stdout","text":"Loaded 1000 tasks\n         h              w            n_colors          \n       min max   mean min max   mean      min max  mean\nrole                                                   \ninput    1  30  11.41   1  30  11.89        1  10  3.81\noutput   1  30   9.95   1  30  10.37        1  10  3.61\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x300 with 2 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAeQAAACCCAYAAACXdLsKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAKl0lEQVR4nO3dX0zV9R/H8dehxBDBkbAU1uRwlLYiYuDkpmOurWGbFTpDTRwWLGCn8gb1ojUPa+sfiv2ZGG0uNTbm2sp0ulm2GpT9XUvMxsDidCFtWpk5j3Shn9+V58fpgKJ89bzR5+PucD5+vp9z4PN98vUAx+eccwIAAEmVkuwFAAAAggwAgAkEGQAAAwgyAAAGEGQAAAwgyAAAGECQAQAwgCADAGAAQQYAwACCDACAAQQ5ybZv3y6fz6fvv/8+2UtRNBpVOBzW559/nuylADeEo0ePqrq6Wnl5eZo8ebJyc3O1cuVKHT169KrnfOmll7R7927vFnkJhw4dUjgc1t9//31djnezI8iIiUajam5uJsiABz744AOVlpbq008/1ZNPPqm2tjbV1tbqs88+U2lpqT788MOrmvd6B7m5uZkgXye3JnsBAHCj+eWXX7Rq1SoVFBSoq6tLOTk5sfvWrFmjYDCoVatWqaenRwUFBUlcKSzhCtmY1atXa+rUqTp+/LgqKys1depU5eTkqKmpSefPn4+Ni0Qi8vl82rhxozZv3qxZs2YpLS1NDzzwgH766ae4ORcsWKAFCxaMeKz8/PzYfBdPGs3NzfL5fPL5fAqHw9fqoQI3rJaWFkWjUb3zzjtxMZak7Oxstbe36+zZs3rttdckxe/F4cLhsHw+X+y2z+fT2bNntWPHjtgeXb16ddzY3t5eVVVVKTMzU9OnT9eaNWs0NDQUm+PiuWP79u0Jxxu+58PhsNauXStJ8vv9seNFIpGrf2JwSVwhG3T+/HlVVFSovLxcGzdu1MGDB7Vp0yYFAgE1NjbGjd25c6fOnDmjUCikoaEhvfHGG3rwwQd15MgR3XHHHWM+Zk5OjrZu3arGxkYtXrxYS5YskSQVFxd7+tiAm8HevXuVn5+vYDA44v3z589Xfn6+9u3bd0Xzvvfee6qrq9O8efP09NNPS5ICgUDcmKqqKuXn5+vll1/W119/rTfffFOnTp3Szp07r+hYS5YsUV9fnzo7O7V582ZlZ2dLUsI3GPAOQTZoaGhIy5Yt0wsvvCBJamhoUGlpqbZt25YQ5GPHjqm/v195eXmSpIULF6q8vFyvvvqqWltbx3zM9PR0LV26VI2NjSouLlZ1dbV3Dwi4iZw+fVqDg4N67LHHLjmuuLhYe/bs0ZkzZ8Y8d3V1tRoaGlRQUDDqHvX7/froo48kSaFQSJmZmWpra1NTU9MVfYNdXFys0tJSdXZ2qrKycsQreHiL/7I2qqGhIe52MBjUr7/+mjCusrIyFmNJmjdvnsrLy7V///5rvkYAiS4GNiMj45LjLt7/zz//eHr8UCgUd/vZZ5+VJM4JEwBBNui2225L+G+hrKwsnTp1KmHsnDlzEj5WWFjI6zxAklwM7eWufMca7iv133NCIBBQSkoK54QJgCAbdMstt3g63/AfChlu+A+JAfDGtGnTNHPmTPX09FxyXE9Pj/Ly8pSZmXlN9+h/5+Z8YBdBnuD6+/sTPtbX1xf3ek9WVtaIv0f422+/xd0ebaMCuDKLFi3SwMCAvvjiixHv7+7uViQS0aJFiySNfY9Kl9+n/z0nHDt2TBcuXIidE7KysiQp4XhXcyx4iyBPcLt379bx48djt7/99lt98803evjhh2MfCwQC6u3t1cmTJ2MfO3z4sL788su4uaZMmSIpcaMCuDJr165VWlqa6uvr9eeff8bd99dff6mhoUFTpkyJ/VpRIBDQ6dOn466qf//99xH/eEh6evol9+iWLVvibr/11luSFDsnZGZmKjs7W11dXXHj2traRjyWxDnheuGnrCe42bNn6/7771djY6P+/fdfvf7665o+fbrWrVsXG/PUU0+ptbVVFRUVqq2t1YkTJ/T222/rnnvuifuBkrS0NN19993atWuXCgsLdfvtt6uoqEhFRUXJeGjAhDVnzhzt2LFDK1eu1L333qva2lr5/X5FIhFt27ZNf/zxhzo7O2O/srR8+XKtX79eixcv1nPPPadoNKqtW7eqsLBQP/zwQ9zcZWVlOnjwoFpbW5Wbmyu/36/y8vLY/QMDA3r00Ue1cOFCffXVV+ro6NATTzyh++67Lzamrq5Or7zyiurq6jR37lx1dXWpr68v4XGUlZVJkp5//nktX75ckyZN0iOPPBILNTzmkFTvvvuuk+S+++4755xzNTU1Lj09PWHchg0b3PBP18DAgJPkWlpa3KZNm9ydd97pJk+e7ILBoDt8+HDCv+/o6HAFBQUuNTXVlZSUuAMHDriamho3a9asuHGHDh1yZWVlLjU11UlyGzZs8PTxAjeTnp4et2LFCjdz5kw3adIkN2PGDLdixQp35MiRhLEff/yxKyoqcqmpqe6uu+5yHR0dCfveOed6e3vd/PnzXVpampPkampqnHP/P0f8/PPPbunSpS4jI8NlZWW5Z555xp07dy5ujmg06mpra920adNcRkaGq6qqcidOnBhxz7/44osuLy/PpaSkOEluYGDAy6cIw/iccy5p3w3gqkUiEfn9frW0tKipqSnZywGQZOFwWM3NzTp58mTsj3hgYuE1ZAAADCDIAAAYQJABADCA15ABADCAK2QAAAwgyAAAGECQAQAwYMx/qeuhlMc9OeCBwR/HPUdFbsm458DE4cXXjGTr6+aTC+8newljYmnfB0P1418IJozuLe2ezDOR9j1XyAAAGECQAQAwgCADAGAAQQYAwACCDACAAQQZAAADCDIAAAYQZAAADCDIAAAYQJABADCAIAMAYABBBgDAAIIMAIABBBkAAAMIMgAABhBkAAAMuHWsAy29SbyltWB01j5PXqznZvua8epzGAzVj3sOr96w3ou1YHTWPk/dg+Nfz/Xa91whAwBgAEEGAMAAggwAgAEEGQAAAwgyAAAGEGQAAAwgyAAAGECQAQAwgCADAGAAQQYAwACCDACAAQQZAAADCDIAAAYQZAAADCDIAAAYQJABADCAIAMAYIDPOefGMvChlMev9VquuwODP3oyT0VuiSfzWOLFc8PzMrqUGf2ezHOtse9HFwzVezKPJd1b2sc9B8/L6C6377lCBgDAAIIMAIABBBkAAAMIMgAABhBkAAAMIMgAABhAkAEAMIAgAwBgAEEGAMAAggwAgAEEGQAAAwgyAAAGEGQAAAwgyAAAGECQAQAwgCADAGCAzznnxjLwRnyjcq948YbnFbkl455D8u7N171aD0b2yYX3k72EMWHfj86LvRYM1Y9/IZK6t7R7Mg/7/tq63L7nChkAAAMIMgAABhBkAAAMIMgAABhAkAEAMIAgAwBgAEEGAMAAggwAgAEEGQAAAwgyAAAGEGQAAAwgyAAAGECQAQAwgCADAGAAQQYAwACCDACAAQQZAAADbk32Am4EFbkl457jwOCP455D8mYtAC7Pi73WPdg+/oWIfX+j4AoZAAADCDIAAAYQZAAADCDIAAAYQJABADCAIAMAYABBBgDAAIIMAIABBBkAAAMIMgAABhBkAAAMIMgAABhAkAEAMIAgAwBgAEEGAMAAggwAgAEEGQAAA3zOOZfsRQAAcLPjChkAAAMIMgAABhBkAAAMIMgAABhAkAEAMIAgAwBgAEEGAMAAggwAgAEEGQAAA/4H2sZxDp54plUAAAAASUVORK5CYII=\n"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"**Baseline Heuristik**","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n\n# ——— ArcDataset (seperti sebelumnya) ———\nclass ArcDataset(Dataset):\n    def __init__(self, challenges_path, solutions_path=None, split='train'):\n        raw_c = json.load(open(challenges_path))\n        if isinstance(raw_c, dict) and 'tasks' in raw_c:\n            self.challenges = raw_c['tasks']\n        elif isinstance(raw_c, dict) and all(isinstance(v, dict) for v in raw_c.values()):\n            self.challenges = []\n            for tid, rec in raw_c.items():\n                rec = rec.copy(); rec['id'] = tid\n                self.challenges.append(rec)\n        else:\n            self.challenges = raw_c\n\n        self.solutions = {}\n        if solutions_path:\n            raw_s = json.load(open(solutions_path))\n            if isinstance(raw_s, dict) and 'tasks' in raw_s:\n                sol_records = raw_s['tasks']\n            elif isinstance(raw_s, dict) and all(isinstance(v, dict) for v in raw_s.values()):\n                sol_records = []\n                for tid, rec in raw_s.items():\n                    rec = rec.copy(); rec['id'] = tid\n                    sol_records.append(rec)\n            elif isinstance(raw_s, list):\n                sol_records = raw_s\n            else:\n                sol_records = []\n\n            for rec in sol_records:\n                tid = rec.get('id') or rec.get('task_id')\n                if tid is not None:\n                    self.solutions[tid] = rec\n\n        self.split = split\n\n    def __len__(self):\n        return len(self.challenges)\n\n    def __getitem__(self, idx):\n        t   = self.challenges[idx]\n        tid = t['id']\n        train_pairs = [(np.array(ex['input']), np.array(ex['output'])) for ex in t['train']]\n        test_inputs = [np.array(ex['input']) for ex in t['test']]\n\n        sample = {'id': tid, 'train': train_pairs, 'test': test_inputs}\n        if self.split in ('train','eval') and tid in self.solutions:\n            sol = self.solutions[tid]\n            if 'test' in sol:\n                sample['test_gt'] = [np.array(ex['output']) for ex in sol['test']]\n        return sample\n\n# ——— DataLoader ———\nDATA_DIR = '/kaggle/input/arc-prize-2025'\neval_ds = ArcDataset(\n    os.path.join(DATA_DIR, 'arc-agi_evaluation_challenges.json'),\n    os.path.join(DATA_DIR, 'arc-agi_evaluation_solutions.json'),\n    split='eval'\n)\neval_loader = DataLoader(eval_ds, batch_size=1, shuffle=False, collate_fn=lambda b: b[0])\n\n# ——— Helpers ———\ndef to_np(x):\n    try:\n        import torch\n        if isinstance(x, torch.Tensor):\n            return x.cpu().numpy()\n    except ImportError:\n        pass\n    return x\n\n# dasar rotasi/flip\ndef identity(x): return x.copy()\ndef rot90(x):    return np.rot90(x, 1)\ndef rot180(x):   return np.rot90(x, 2)\ndef rot270(x):   return np.rot90(x, 3)\ndef flip_h(x):   return np.fliplr(x)\ndef flip_v(x):   return np.flipud(x)\n\nOPS = {\n    'identity': identity,\n    'rot90'   : rot90,\n    'rot180'  : rot180,\n    'rot270'  : rot270,\n    'flip_h'  : flip_h,\n    'flip_v'  : flip_v\n}\n\n# deteksi rotasi/flip\ndef detect_op(pairs):\n    for name, op in OPS.items():\n        if all(np.array_equal(op(to_np(inp)), to_np(out)) for inp,out in pairs):\n            return op, name\n    return identity, 'identity'\n\n# operasi fill\ndef fill(x, fill_color):\n    vals, counts = np.unique(x, return_counts=True)\n    bg = vals[np.argmax(counts)]\n    y = x.copy()\n    y[x == bg] = fill_color\n    return y\n\n# deteksi color-swap mapping\ndef detect_color_swap(pairs):\n    mapping = {}\n    for inp, out in pairs:\n        inp_np, out_np = to_np(inp), to_np(out)\n        if inp_np.shape != out_np.shape:\n            return None\n        for i in range(inp_np.shape[0]):\n            for j in range(inp_np.shape[1]):\n                ci, co = inp_np[i,j], out_np[i,j]\n                if ci == co: continue\n                if ci in mapping and mapping[ci] != co:\n                    return None\n                mapping[ci] = co\n    return mapping if mapping else None\n\n# deteksi extended: rot/flip → fill → color-swap\ndef detect_op_extended(pairs):\n    # 1. rotasi/flip\n    op, name = detect_op(pairs)\n    if name != 'identity':\n        return op, name\n\n    # 2. fill\n    # cari warna baru\n    fill_cands = []\n    for inp,out in pairs:\n        inp_np, out_np = to_np(inp), to_np(out)\n        new_cols = set(np.unique(out_np)) - set(np.unique(inp_np))\n        if len(new_cols) == 1:\n            fill_cands.append(next(iter(new_cols)))\n        else:\n            fill_cands = []\n            break\n    if fill_cands and len(set(fill_cands)) == 1:\n        fc = fill_cands[0]\n        return (lambda x: fill(x, fc)), f'fill({fc})'\n\n    # 3. color-swap\n    mapping = detect_color_swap(pairs)\n    if mapping is not None:\n        return (lambda x: np.vectorize(lambda c: mapping.get(c, c))(to_np(x))), f'swap({mapping})'\n\n    return identity, 'identity'\n\n# ——— Loop Evaluasi Extended Baseline ———\ncorrect = total = 0\nfor sample in tqdm(eval_loader, desc='Extended baseline eval'):\n    op_func, op_name = detect_op_extended(sample['train'])\n    preds = [op_func(to_np(inp)) for inp in sample['test']]\n    gts   = sample.get('test_gt', [])\n    if gts and all(np.array_equal(p, to_np(gt)) for p,gt in zip(preds, gts)):\n        correct += 1\n    total += 1\n\nprint(f'Extended baseline accuracy: {correct}/{total} = {correct/total:.2%}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:08:38.866474Z","iopub.execute_input":"2025-07-17T19:08:38.867007Z","iopub.status.idle":"2025-07-17T19:08:39.243528Z","shell.execute_reply.started":"2025-07-17T19:08:38.866983Z","shell.execute_reply":"2025-07-17T19:08:39.242566Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Extended baseline eval:   0%|          | 0/120 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6af3fd876d74cc78a71f21bf2364fb1"}},"metadata":{}},{"name":"stdout","text":"Extended baseline accuracy: 0/120 = 0.00%\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Perception Encoder","metadata":{}},{"cell_type":"code","source":"# ─── 2) Perception Encoder (with dynamic padding) ─────────────────────────────\n\nimport os, json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n\n# -------------------------------------------------------------------\n# (1) ArcDataset: pastikan definisi ini sudah dijalankan di atas\n# -------------------------------------------------------------------\n# class ArcDataset: ...\n\n# -------------------------------------------------------------------\n# (2) Dataset untuk Training Encoder\n# -------------------------------------------------------------------\nclass EncoderTrainDataset(Dataset):\n    def __init__(self, arc_ds):\n        self.pairs = []\n        for sample in arc_ds:\n            for inp, out in sample['train']:\n                self.pairs.append((inp, out))\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        inp, out = self.pairs[idx]\n        return torch.from_numpy(inp).long(), torch.from_numpy(out).long()\n\n# -------------------------------------------------------------------\n# ─── Collate_fn: pad inp & out secara terpisah ─────────────────────────────────\ndef pad_collate(batch):\n    inps, outs = zip(*batch)\n    # cari dimensi maksimum di antara semua inp dan semua out\n    max_h = max(max(t.shape[0] for t in inps), max(t.shape[0] for t in outs))\n    max_w = max(max(t.shape[1] for t in inps), max(t.shape[1] for t in outs))\n\n    pad_inps, pad_outs = [], []\n    for inp, out in zip(inps, outs):\n        hi, wi = inp.shape\n        ho, wo = out.shape\n        # hitung padding terpisah\n        pad_i = (0, max_w - wi, 0, max_h - hi)  # (left, right, top, bottom)\n        pad_o = (0, max_w - wo, 0, max_h - ho)\n        pad_inps.append(F.pad(inp, pad_i, value=0))\n        pad_outs.append(F.pad(out, pad_o, value=0))\n\n    batch_inp = torch.stack(pad_inps, dim=0)\n    batch_out = torch.stack(pad_outs, dim=0)\n    return batch_inp, batch_out\n\n# Pastikan DataLoader-mu memakai collate_fn ini:\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=32,\n    shuffle=True,\n    num_workers=0,\n    collate_fn=pad_collate\n)\n# -------------------------------------------------------------------\n# (4) Model: CNN‐based Siamese Encoder\n# -------------------------------------------------------------------\nclass PerceptionEncoderCNN(nn.Module):\n    def __init__(self, num_colors=11, emb_dim=128):\n        super().__init__()\n        self.embed = nn.Embedding(num_colors, 16)\n        self.conv1 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.pool  = nn.AdaptiveAvgPool2d((1,1))\n        self.fc    = nn.Linear(64, emb_dim)\n\n    def forward(self, grids):\n        x = self.embed(grids)                  # (B, H, W, 16)\n        x = x.permute(0, 3, 1, 2).contiguous()  # (B,16,H,W)\n        x = F.relu(self.conv1(x))              # (B,32,H,W)\n        x = F.relu(self.conv2(x))              # (B,64,H,W)\n        x = self.pool(x)                       # (B,64,1,1)\n        x = x.view(x.size(0), -1)              # (B,64)\n        return self.fc(x)                      # (B,emb_dim)\n\n# -------------------------------------------------------------------\n# (5) Setup: load data, buat DataLoader, instansiasi model & optimizer\n# -------------------------------------------------------------------\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nDATA_DIR = '/kaggle/input/arc-prize-2025'\narc_ds = ArcDataset(\n    os.path.join(DATA_DIR, 'arc-agi_training_challenges.json'),\n    os.path.join(DATA_DIR, 'arc-agi_training_solutions.json'),\n    split='train'\n)\ntrain_ds = EncoderTrainDataset(arc_ds)\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=32,\n    shuffle=True,\n    num_workers=0,       # ubah ke 0 kalau masih ada masalah worker\n    collate_fn=pad_collate\n)\n\nencoder = PerceptionEncoderCNN(num_colors=11, emb_dim=128).to(device)\nopt     = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n\ndef siamese_loss(e1, e2):\n    cos = F.cosine_similarity(e1, e2, dim=-1)\n    return (1.0 - cos).mean()\n\n# -------------------------------------------------------------------\n# (6) Training Loop\n# -------------------------------------------------------------------\nepochs = 20\nfor ep in range(1, epochs+1):\n    encoder.train()\n    total_loss = 0\n    for inp, out in tqdm(train_loader, desc=f\"Epoch {ep}/{epochs}\"):\n        inp, out = inp.to(device), out.to(device)\n        e_in  = encoder(inp)\n        e_out = encoder(out)\n        loss  = siamese_loss(e_in, e_out)\n\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        total_loss += loss.item() * inp.size(0)\n\n    avg = total_loss / len(train_loader.dataset)\n    print(f\"[Epoch {ep:02d}] Avg Loss: {avg:.4f}\")\n\n# ─── (7) Save & Smoke Test with padding ───────────────────────────────────────\n\n# fungsi util untuk pad list of grids saja\ndef pad_grids(grids):\n    \"\"\"\n    grids: list of 2D numpy arrays\n    returns: LongTensor (B, Hmax, Wmax) padded with 0\n    \"\"\"\n    # ubah jadi tensor list\n    ts = [torch.from_numpy(g).long() for g in grids]\n    max_h = max(t.shape[0] for t in ts)\n    max_w = max(t.shape[1] for t in ts)\n\n    padded = []\n    for t in ts:\n        h, w = t.shape\n        pad = (0, max_w - w, 0, max_h - h)  # (left, right, top, bottom)\n        padded.append(F.pad(t, pad, value=0))\n    return torch.stack(padded, dim=0)\n\n# Save checkpoint\ntorch.save(encoder.state_dict(), \"perception_encoder_cnn.pth\")\nprint(\"✅ Checkpoint saved.\")\n\n# Smoke test: pad semua grid sebelum ke encoder\nencoder.eval()\nsample = arc_ds[0]\n# ambil semua grid (train inp/out + test inp)\ngrids = [g for inp,out in sample['train'] for g in (inp, out)] + sample['test']\nbatch = pad_grids(grids).to(device)        # <<< gunakan padding\nwith torch.no_grad():\n    embs = encoder(batch)\nprint(\"Smoke test embeddings shape:\", embs.shape)  # expect (num_grids, 128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:14:35.363336Z","iopub.execute_input":"2025-07-17T19:14:35.363716Z","iopub.status.idle":"2025-07-17T19:14:48.555452Z","shell.execute_reply.started":"2025-07-17T19:14:35.363691Z","shell.execute_reply":"2025-07-17T19:14:48.554808Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55811355c4314fa9928e3a88fc103458"}},"metadata":{}},{"name":"stdout","text":"[Epoch 01] Avg Loss: 0.0001\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a60bb3f18842432f89366bf0ae6a589b"}},"metadata":{}},{"name":"stdout","text":"[Epoch 02] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a6eaceb098b47c6ab4ad0953bc8ef1d"}},"metadata":{}},{"name":"stdout","text":"[Epoch 03] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d8cbbbcdd514651a3f3d3466ee61a2b"}},"metadata":{}},{"name":"stdout","text":"[Epoch 04] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"650cd37352354acba6e89494d36120be"}},"metadata":{}},{"name":"stdout","text":"[Epoch 05] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce4f2a17a40949d19452d14be7aec8fd"}},"metadata":{}},{"name":"stdout","text":"[Epoch 06] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f474e14c0ed34749b7688947769faa65"}},"metadata":{}},{"name":"stdout","text":"[Epoch 07] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eebf287471e54c6387a6ebfcccf90370"}},"metadata":{}},{"name":"stdout","text":"[Epoch 08] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c57034ae81484676b8a01766be6abdef"}},"metadata":{}},{"name":"stdout","text":"[Epoch 09] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a888bfed6e7d4a95a0052afc88a61812"}},"metadata":{}},{"name":"stdout","text":"[Epoch 10] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdae8e81f9b94f2ba25fec611b1ff15f"}},"metadata":{}},{"name":"stdout","text":"[Epoch 11] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61954041b75e41bc9915fb4c1161c95c"}},"metadata":{}},{"name":"stdout","text":"[Epoch 12] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fc64e6ad3bf4ceaa8e828ada1fa4afa"}},"metadata":{}},{"name":"stdout","text":"[Epoch 13] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00c8f9263adf4a5fa1d7cd4cffaeab5e"}},"metadata":{}},{"name":"stdout","text":"[Epoch 14] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cd0436b9c8344e896f26d7106c1e4ca"}},"metadata":{}},{"name":"stdout","text":"[Epoch 15] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 16/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e51ca1064f64468a6a4aa688506c42a"}},"metadata":{}},{"name":"stdout","text":"[Epoch 16] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 17/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dbcd9f9af354c8199670fcf79c98bc1"}},"metadata":{}},{"name":"stdout","text":"[Epoch 17] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 18/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3e381a173294d4fb2ee1388d2786aca"}},"metadata":{}},{"name":"stdout","text":"[Epoch 18] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 19/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29ca020ff4df4f589aa27355edb4e72b"}},"metadata":{}},{"name":"stdout","text":"[Epoch 19] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 20/20:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e2474dbfb3747d69ccaf8af4dce43c8"}},"metadata":{}},{"name":"stdout","text":"[Epoch 20] Avg Loss: 0.0000\n✅ Checkpoint saved.\nSmoke test embeddings shape: torch.Size([5, 128])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Desain DSL & Beam-Search","metadata":{}},{"cell_type":"code","source":"# ─── 2) DSL Primitives & Safe Scoring with Neural Guidance (Revised) ─────────────\n\nimport os, json, numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# ————————————————————————————————————————————————————————————————\n# (1) Definisikan ulang PerceptionEncoderCNN & load checkpoint\n# ————————————————————————————————————————————————————————————————\nclass PerceptionEncoderCNN(nn.Module):\n    def __init__(self, num_colors=11, emb_dim=128):\n        super().__init__()\n        self.embed = nn.Embedding(num_colors, 16)\n        self.conv1 = nn.Conv2d(16, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool  = nn.AdaptiveAvgPool2d((1,1))\n        self.fc    = nn.Linear(64, emb_dim)\n\n    def forward(self, grids):\n        x = self.embed(grids)\n        x = x.permute(0,3,1,2).contiguous()\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        return self.fc(x)\n\n# Load encoder\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nencoder = PerceptionEncoderCNN(num_colors=11, emb_dim=128).to(device)\nencoder.load_state_dict(torch.load('perception_encoder_cnn.pth', map_location=device))\nencoder.eval()\n\n# ————————————————————————————————————————————————————————————————\n# (2) embed_grid util (pastikan grid kontigu)\n# ————————————————————————————————————————————————————————————————\ndef embed_grid(grid: np.ndarray) -> torch.Tensor:\n    # copy to remove negative strides (e.g. from rot/flips)\n    arr = np.ascontiguousarray(grid)\n    t   = torch.from_numpy(arr).long().unsqueeze(0).to(device)  # (1,H,W)\n    with torch.no_grad():\n        e = encoder(t)  # (1,emb_dim)\n    return F.normalize(e.squeeze(0), dim=-1)  # (emb_dim,)\n\n# ————————————————————————————————————————————————————————————————\n# (3) DSL Primitives\n# ————————————————————————————————————————————————————————————————\ndef identity(x):    return x.copy()\ndef rot90(x):       return np.rot90(x, 1)\ndef rot180(x):      return np.rot90(x, 2)\ndef rot270(x):      return np.rot90(x, 3)\ndef flip_h(x):      return np.fliplr(x)\ndef flip_v(x):      return np.flipud(x)\ndef shift(x, dx, dy): return np.roll(np.roll(x, dy, axis=0), dx, axis=1)\ndef fill(x, c):\n    vals, counts = np.unique(x, return_counts=True)\n    bg = vals[np.argmax(counts)]\n    y = x.copy(); y[x==bg] = c\n    return y\n\nBASE_OPS = {\n    'identity': identity,\n    'rot90':    rot90,\n    'rot180':   rot180,\n    'rot270':   rot270,\n    'flip_h':   flip_h,\n    'flip_v':   flip_v,\n}\n\n# ————————————————————————————————————————————————————————————————\n# (4) Program & Combined Scoring\n# ————————————————————————————————————————————————————————————————\nclass Program:\n    def __init__(self, ops=None):\n        self.ops = ops or []\n    def apply(self, grid):\n        out = grid.copy()\n        for _, fn in self.ops:\n            out = fn(out)\n        return out\n    def extend(self, name, fn):\n        return Program(self.ops + [(name, fn)])\n    def __repr__(self):\n        return ' -> '.join(name for name,_ in self.ops) or 'identity'\n\ndef score_program(prog, pairs, α=0.7):\n    correct_pix, total_pix, sim_total = 0, 0, 0.0\n    for inp, tgt in pairs:\n        pred = prog.apply(inp)\n        if pred.shape == tgt.shape:\n            correct_pix += np.sum(pred == tgt)\n        total_pix += tgt.size\n        e_pred = embed_grid(pred)\n        e_tgt  = embed_grid(tgt)\n        sim_total += float(torch.dot(e_pred, e_tgt))\n    pix_score = correct_pix / total_pix\n    sim_score = sim_total / len(pairs)\n    return α * pix_score + (1-α) * sim_score\n\n# ————————————————————————————————————————————————————————————————\n# (5) Beam Search Synthesizer\n# ————————————————————————————————————————————————————————————————\ndef synthesize_program(pairs, beam_width=20, max_depth=5, α=0.7):\n    beam = [Program()]\n    best_prog, best_score = beam[0], score_program(beam[0], pairs, α)\n\n    inp_cols = set().union(*(np.unique(inp) for inp,_ in pairs))\n    tgt_cols = set().union(*(np.unique(tgt) for _,tgt in pairs))\n    fills    = list(tgt_cols - inp_cols)\n\n    for _ in range(max_depth):\n        new_cands = []\n        for prog in beam:\n            for name, fn in BASE_OPS.items():\n                p2 = prog.extend(name, fn)\n                sc = score_program(p2, pairs, α)\n                new_cands.append((sc, p2))\n                if sc > best_score:\n                    best_score, best_prog = sc, p2\n                    if best_score >= 1.0:\n                        return best_prog\n            for c in fills:\n                fn = lambda x, c=c: fill(x, c)\n                p2 = prog.extend(f'fill({c})', fn)\n                new_cands.append((score_program(p2, pairs, α), p2))\n            for dx in (-1,1):\n                for dy in (-1,1):\n                    fn = lambda x, dx=dx, dy=dy: shift(x, dx, dy)\n                    name = f'shift({dx},{dy})'\n                    p2 = prog.extend(name, fn)\n                    new_cands.append((score_program(p2, pairs, α), p2))\n        new_cands.sort(key=lambda x: x[0], reverse=True)\n        beam = [p for _, p in new_cands[:beam_width]]\n\n    return best_prog\n\n# ————————————————————————————————————————————————————————————————\n# (6) Contoh Panggilan\n# ————————————————————————————————————————————————————————————————\nDATA_DIR = '/kaggle/input/arc-prize-2025'\nds = ArcDataset(\n    os.path.join(DATA_DIR, 'arc-agi_training_challenges.json'),\n    os.path.join(DATA_DIR, 'arc-agi_training_solutions.json'),\n    split='train'\n)\ntrain_pairs = ds[0]['train']\nbest_prog = synthesize_program(train_pairs)\nprint(\"Best program:\", best_prog)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:18:36.259763Z","iopub.execute_input":"2025-07-17T19:18:36.260062Z","iopub.status.idle":"2025-07-17T19:18:37.972851Z","shell.execute_reply.started":"2025-07-17T19:18:36.260041Z","shell.execute_reply":"2025-07-17T19:18:37.972100Z"}},"outputs":[{"name":"stdout","text":"Best program: rot270 -> flip_v\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ─── Evaluate Beam-Search Synthesizer on Public Eval (Revised) ───────────────\n\nimport os\nimport json\nimport numpy as np\nfrom tqdm.auto import tqdm\n\n# 1) Sesuaikan path ke folder data jika perlu\nDATA_DIR = '/kaggle/input/arc-prize-2025'\n# Jika ERROR “No such file or directory”, ubah DATA_DIR ke '../input/arc-prize-2025'\n\n# 2) Muat evaluation dataset\neval_challenges = os.path.join(DATA_DIR, 'arc-agi_evaluation_challenges.json')\neval_solutions  = os.path.join(DATA_DIR, 'arc-agi_evaluation_solutions.json')\neval_ds = ArcDataset(eval_challenges, eval_solutions, split='eval')\n\n# 3) Parameter baru hasil tuning\nbeam_width = 20\nmax_depth  = 5\nalpha      = 0.7\n\n# 4) Loop evaluasi dengan tracking errors\ncorrect = 0\nerrors  = []\nfor sample in tqdm(eval_ds, desc='Synth eval'):\n    tid = sample['id']\n    try:\n        prog = synthesize_program(\n            sample['train'],\n            beam_width=beam_width,\n            max_depth=max_depth,\n            α=alpha\n        )\n        preds = [prog.apply(inp) for inp in sample['test']]\n        gts   = sample.get('test_gt', [])\n\n        # cek match hanya jika shapes sama *dan* semua elemen sama\n        if gts and all(\n            (p.shape == gt.shape and np.array_equal(p, gt))\n            for p,gt in zip(preds, gts)\n        ):\n            correct += 1\n    except Exception as e:\n        errors.append((tid, str(e)))\n\n# 5) Tampilkan akurasi dan ringkasan error\ntotal = len(eval_ds)\nprint(f'Beam-search accuracy: {correct}/{total} = {correct/total:.2%}')\nif errors:\n    print(f'– Encountered errors on {len(errors)} tasks, e.g.:', errors[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:19:32.644035Z","iopub.execute_input":"2025-07-17T19:19:32.644952Z","iopub.status.idle":"2025-07-17T19:24:21.995987Z","shell.execute_reply.started":"2025-07-17T19:19:32.644914Z","shell.execute_reply":"2025-07-17T19:24:21.995136Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Synth eval:   0%|          | 0/120 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b3a7ba9a2d64837ba1cde2a34d733b7"}},"metadata":{}},{"name":"stdout","text":"Beam-search accuracy: 0/120 = 0.00%\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Perception Encoder","metadata":{}},{"cell_type":"code","source":"# ─── Perception Encoder with Training Loop ────────────────────────────────────\n\nimport os, json\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\n# -------------------------------------------------------------------\n# (1) ArcDataset: pastikan definisi kamu sudah di atas\n# -------------------------------------------------------------------\n# class ArcDataset(Dataset): ...\n\n# -------------------------------------------------------------------\n# (2) Dataset khusus untuk melatih encoder (Siamese pairs)\n# -------------------------------------------------------------------\nclass EncoderTrainDataset(Dataset):\n    def __init__(self, arc_ds):\n        self.pairs = []\n        for sample in arc_ds:\n            for inp, out in sample['train']:\n                self.pairs.append((inp, out))\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        inp, out = self.pairs[idx]\n        # convert ke tensor Long\n        return torch.from_numpy(inp).long(), torch.from_numpy(out).long()\n\n# -------------------------------------------------------------------\n# (3) Collate fn untuk pairs (padding dinamis)\n# -------------------------------------------------------------------\ndef pad_collate_pairs(batch):\n    inps, outs = zip(*batch)\n    # cari ukuran max\n    max_h = max(max(t.shape[0] for t in inps), max(t.shape[0] for t in outs))\n    max_w = max(max(t.shape[1] for t in inps), max(t.shape[1] for t in outs))\n    pad_inps, pad_outs = [], []\n    for inp, out in zip(inps, outs):\n        hi, wi = inp.shape; ho, wo = out.shape\n        pad_i = (0, max_w-wi, 0, max_h-hi)\n        pad_o = (0, max_w-wo, 0, max_h-ho)\n        pad_inps.append(F.pad(inp, pad_i, value=0))\n        pad_outs.append(F.pad(out, pad_o, value=0))\n    return torch.stack(pad_inps), torch.stack(pad_outs)\n\n# -------------------------------------------------------------------\n# (4) Model CNN‐Siamese untuk Perception Encoder\n# -------------------------------------------------------------------\nclass PerceptionEncoder(nn.Module):\n    def __init__(self, num_colors=11, color_emb_dim=16, hidden_dim=128):\n        super().__init__()\n        self.color_emb = nn.Embedding(num_colors, color_emb_dim)\n        self.conv = nn.Sequential(\n            nn.Conv2d(color_emb_dim, 32, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(32, 64, 3, padding=1),            nn.ReLU(),\n            nn.Conv2d(64, hidden_dim, 3, padding=1),    nn.ReLU(),\n        )\n        self.pool = nn.AdaptiveAvgPool2d((1,1))\n\n    def forward(self, grids):\n        # grids: (B, H, W) long\n        x = self.color_emb(grids)          # (B, H, W, emb)\n        x = x.permute(0,3,1,2).contiguous()# (B, emb, H, W)\n        f = self.conv(x)                   # (B, hidden, H, W)\n        p = self.pool(f).view(f.size(0), -1) # (B, hidden)\n        return p\n\n# -------------------------------------------------------------------\n# (5) Siapkan DataLoader & Model\n# -------------------------------------------------------------------\nDATA_DIR = '/kaggle/input/arc-prize-2025'\narc_ds = ArcDataset(\n    os.path.join(DATA_DIR, 'arc-agi_training_challenges.json'),\n    os.path.join(DATA_DIR, 'arc-agi_training_solutions.json'),\n    split='train'\n)\ntrain_ds    = EncoderTrainDataset(arc_ds)\ntrain_loader= DataLoader(\n    train_ds, batch_size=64, shuffle=True,\n    num_workers=0, collate_fn=pad_collate_pairs\n)\n\nencoder   = PerceptionEncoder(num_colors=11, color_emb_dim=16, hidden_dim=128).to(device)\noptimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n\n# loss siamese: minimalisir 1 - cosine_similarity\ndef siamese_loss(e1, e2):\n    cos = F.cosine_similarity(e1, e2, dim=-1)\n    return (1.0 - cos).mean()\n\n# -------------------------------------------------------------------\n# (6) Training Loop\n# -------------------------------------------------------------------\nepochs = 20\nfor epoch in range(1, epochs+1):\n    encoder.train()\n    running = 0.0\n    for inp, out in train_loader:\n        inp, out = inp.to(device), out.to(device)\n        e_in  = encoder(inp)\n        e_out = encoder(out)\n        loss  = siamese_loss(e_in, e_out)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        running += loss.item() * inp.size(0)\n\n    avg = running / len(train_loader.dataset)\n    print(f\"[Epoch {epoch:02d}] Avg Siamese Loss: {avg:.4f}\")\n\n# simpan model\ntorch.save(encoder.state_dict(), \"perception_encoder_trained.pth\")\nprint(\"Encoder trained and saved.\")\n\n# -------------------------------------------------------------------\n# (7) Smoke‐test embedding untuk satu sample\n# -------------------------------------------------------------------\nencoder.eval()\nsample = arc_ds[0]\n# collect train inp+out + satu test inp\ngrids = [g for inp,out in sample['train'] for g in (inp, out)] + [sample['test'][0]]\n# pad & to tensor\ndef pad_grids_np(grids):\n    max_h = max(g.shape[0] for g in grids)\n    max_w = max(g.shape[1] for g in grids)\n    batch = np.zeros((len(grids), max_h, max_w), dtype=np.int64)\n    for i,g in enumerate(grids):\n        batch[i,:g.shape[0],:g.shape[1]] = g\n    return torch.from_numpy(batch)\n\nbatch = pad_grids_np(grids).to(device)\nwith torch.no_grad():\n    embs = encoder(batch)\nprint(\"Smoke test embeddings shape:\", embs.shape)  # expect (N, hidden_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:27:26.424208Z","iopub.execute_input":"2025-07-17T19:27:26.424993Z","iopub.status.idle":"2025-07-17T19:27:48.203885Z","shell.execute_reply.started":"2025-07-17T19:27:26.424967Z","shell.execute_reply":"2025-07-17T19:27:48.203106Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n[Epoch 01] Avg Siamese Loss: 0.0001\n[Epoch 02] Avg Siamese Loss: 0.0000\n[Epoch 03] Avg Siamese Loss: 0.0000\n[Epoch 04] Avg Siamese Loss: 0.0000\n[Epoch 05] Avg Siamese Loss: 0.0000\n[Epoch 06] Avg Siamese Loss: 0.0000\n[Epoch 07] Avg Siamese Loss: 0.0000\n[Epoch 08] Avg Siamese Loss: 0.0000\n[Epoch 09] Avg Siamese Loss: 0.0000\n[Epoch 10] Avg Siamese Loss: 0.0000\n[Epoch 11] Avg Siamese Loss: 0.0000\n[Epoch 12] Avg Siamese Loss: 0.0000\n[Epoch 13] Avg Siamese Loss: 0.0000\n[Epoch 14] Avg Siamese Loss: 0.0000\n[Epoch 15] Avg Siamese Loss: 0.0000\n[Epoch 16] Avg Siamese Loss: 0.0000\n[Epoch 17] Avg Siamese Loss: 0.0000\n[Epoch 18] Avg Siamese Loss: 0.0000\n[Epoch 19] Avg Siamese Loss: 0.0000\n[Epoch 20] Avg Siamese Loss: 0.0000\nEncoder trained and saved.\nSmoke test embeddings shape: torch.Size([5, 128])\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Neuro-Symbolic Pipeline","metadata":{}},{"cell_type":"code","source":"# ─── Neuro-Symbolic Pipeline (Revised) ─────────────────────────────────────\nimport os, json, numpy as np, torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\n# 1) Robust ARC Dataset Loader\nclass ArcDataset(Dataset):\n    def __init__(self, challenges_path, solutions_path=None, split='train'):\n        raw = json.load(open(challenges_path))\n        if isinstance(raw, dict) and 'tasks' in raw:\n            tasks = raw['tasks']\n        elif isinstance(raw, list):\n            tasks = []\n            for rec in raw:\n                if isinstance(rec, dict):\n                    rec = rec.copy()\n                    if 'id' not in rec:\n                        rec['id'] = rec.get('task_id')\n                    tasks.append(rec)\n        elif isinstance(raw, dict):\n            tasks = []\n            for tid, rec in raw.items():\n                if isinstance(rec, dict):\n                    rec = rec.copy(); rec['id'] = tid\n                    tasks.append(rec)\n        else:\n            tasks = raw\n        self.challenges = tasks\n\n        self.solutions = {}\n        if solutions_path and os.path.exists(solutions_path):\n            raw_s = json.load(open(solutions_path))\n            sol_list = []\n            if isinstance(raw_s, dict) and 'tasks' in raw_s and isinstance(raw_s['tasks'], list):\n                sol_list = raw_s['tasks']\n            elif isinstance(raw_s, dict):\n                for tid, rec in raw_s.items():\n                    if isinstance(rec, dict):\n                        rec = rec.copy(); rec['id'] = rec.get('id', rec.get('task_id', tid))\n                        sol_list.append(rec)\n            elif isinstance(raw_s, list):\n                for rec in raw_s:\n                    if isinstance(rec, dict):\n                        rec = rec.copy()\n                        if 'id' not in rec and 'task_id' in rec:\n                            rec['id'] = rec['task_id']\n                        sol_list.append(rec)\n            for rec in sol_list:\n                tid = rec.get('id')\n                if tid is not None:\n                    self.solutions[tid] = rec\n        self.split = split\n\n    def __len__(self):\n        return len(self.challenges)\n\n    def __getitem__(self, idx):\n        t = self.challenges[idx]\n        tid = t.get('id', t.get('task_id'))\n        train_pairs = [(np.array(ex['input']), np.array(ex['output'])) for ex in t['train']]\n        test_inputs = [np.array(ex['input']) for ex in t['test']]\n        sample = {'id': tid, 'train': train_pairs, 'test': test_inputs}\n        if self.split in ('train','eval') and tid in self.solutions:\n            sol = self.solutions[tid]\n            if 'test' in sol:\n                sample['test_gt'] = [np.array(ex['output']) for ex in sol['test']]\n        return sample\n\n# Utility: pad list of grids to uniform tensor\ndef pad_grids(grids, pad_value=0):\n    max_h = max(g.shape[0] for g in grids)\n    max_w = max(g.shape[1] for g in grids)\n    batch = np.full((len(grids), max_h, max_w), pad_value, dtype=np.int64)\n    for i, g in enumerate(grids):\n        h, w = g.shape; batch[i, :h, :w] = g\n    return torch.from_numpy(batch)\n\n# 2) Load & Prepare Perception Encoder\nclass PerceptionEncoder(nn.Module):\n    def __init__(self, num_colors=11, color_emb_dim=16, hidden_dim=128):\n        super().__init__()\n        self.color_emb = nn.Embedding(num_colors, color_emb_dim)\n        self.conv = nn.Sequential(\n            nn.Conv2d(color_emb_dim, 32, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n            nn.Conv2d(64, hidden_dim, 3, padding=1), nn.ReLU(),\n        )\n        self.pool = nn.AdaptiveAvgPool2d((1,1))\n\n    def forward(self, grids):\n        x = self.color_emb(grids)                 # (B, H, W, emb)\n        x = x.permute(0,3,1,2).contiguous()       # (B,emb,H,W)\n        f = self.conv(x)                          # (B,hidden,H,W)\n        return self.pool(f).view(f.size(0), -1)   # (B, hidden)\n\n# Instantiate and load trained encoder\nencoder = PerceptionEncoder(num_colors=11, color_emb_dim=16, hidden_dim=128).to(device)\nencoder.load_state_dict(torch.load('perception_encoder_trained.pth', map_location=device))\nencoder.eval()\n\n# Utility to embed a single grid\ndef embed_grid(grid: np.ndarray) -> torch.Tensor:\n    arr = np.ascontiguousarray(grid)\n    t = torch.from_numpy(arr).long().unsqueeze(0).to(device)\n    with torch.no_grad():\n        e = encoder(t)\n    return F.normalize(e.squeeze(0), dim=-1)\n\n# 3) DSL Primitives\ndef identity(x):    return x.copy()\ndef rot90(x):       return np.rot90(x, 1)\ndef rot180(x):      return np.rot90(x, 2)\ndef rot270(x):      return np.rot90(x, 3)\ndef flip_h(x):      return np.fliplr(x)\ndef flip_v(x):      return np.flipud(x)\ndef shift(x, dx, dy): return np.roll(np.roll(x, dy, axis=0), dx, axis=1)\ndef fill(x, c):\n    vals, counts = np.unique(x, return_counts=True)\n    bg = vals[np.argmax(counts)]\n    y = x.copy(); y[x==bg] = c\n    return y\nBASE_OPS = {\n    'identity': identity, 'rot90': rot90, 'rot180': rot180,\n    'rot270': rot270, 'flip_h': flip_h, 'flip_v': flip_v\n}\n\n# 4) Program & Combined Scoring\nclass Program:\n    def __init__(self, ops=None):\n        self.ops = ops or []\n\n    def apply(self, grid):\n        out = grid.copy()\n        for _, fn in self.ops:\n            out = fn(out)\n        return out\n\n    def extend(self, name, fn):\n        return Program(self.ops + [(name, fn)])\n\n    def __repr__(self):\n        return ' -> '.join(name for name, _ in self.ops) or 'identity'\n\ndef score_program(prog, pairs, alpha=0.7):\n    pix_corr, pix_tot, sim = 0, 0, 0.0\n    for inp, tgt in pairs:\n        pred = prog.apply(inp)\n        if pred.shape == tgt.shape:\n            pix_corr += np.sum(pred == tgt)\n        pix_tot += tgt.size\n        sim += float(torch.dot(embed_grid(pred), embed_grid(tgt)))\n    pix_score = pix_corr / pix_tot\n    sim_score = sim / len(pairs)\n    return alpha * pix_score + (1-alpha) * sim_score\n\n# 5) Beam-Search Synthesizer\ndef synthesize_program(pairs, beam_width=20, max_depth=5, alpha=0.7):\n    beam = [Program()]\n    best_prog = beam[0]\n    best_score = score_program(best_prog, pairs, alpha)\n    inp_cols = set().union(*(np.unique(i) for i,_ in pairs))\n    tgt_cols = set().union(*(np.unique(o) for _,o in pairs))\n    fills = list(tgt_cols - inp_cols)\n\n    for _ in range(max_depth):\n        candidates = []\n        for prog in beam:\n            for name, fn in BASE_OPS.items():\n                p2 = prog.extend(name, fn)\n                sc = score_program(p2, pairs, alpha)\n                candidates.append((sc, p2))\n                if sc > best_score:\n                    best_score, best_prog = sc, p2\n                    if best_score >= 1.0:\n                        return best_prog\n            for c in fills:\n                fn = lambda x, c=c: fill(x, c)\n                p2 = prog.extend(f'fill({c})', fn)\n                candidates.append((score_program(p2, pairs, alpha), p2))\n            for dx in (-1,1):\n                for dy in (-1,1):\n                    fn = lambda x, dx=dx, dy=dy: shift(x, dx, dy)\n                    p2 = prog.extend(f'shift({dx},{dy})', fn)\n                    candidates.append((score_program(p2, pairs, alpha), p2))\n        candidates.sort(key=lambda x: x[0], reverse=True)\n        beam = [p for _, p in candidates[:beam_width]]\n    return best_prog\n\n# 6) Neuro-Symbolic Solver\nclass NeuroSymbolicSolver:\n    def __init__(self, encoder, beam_width=20, max_depth=5, alpha=0.7):\n        self.encoder = encoder\n        self.beam_width = beam_width\n        self.max_depth = max_depth\n        self.alpha = alpha\n\n    def solve(self, train_pairs, test_inputs):\n        prog = synthesize_program(train_pairs, self.beam_width, self.max_depth, self.alpha)\n        preds = [prog.apply(inp) for inp in test_inputs]\n        return preds, prog\n\n# 7) Evaluate on Public Eval\nDATA_DIR = '/kaggle/input/arc-prize-2025'\neval_ds = ArcDataset(\n    os.path.join(DATA_DIR,'arc-agi_evaluation_challenges.json'),\n    os.path.join(DATA_DIR,'arc-agi_evaluation_solutions.json'), split='eval')\nsolver = NeuroSymbolicSolver(encoder)\ncorrect = 0\nfor sample in eval_ds:\n    preds, _ = solver.solve(sample['train'], sample['test'])\n    gts = sample.get('test_gt', [])\n    if gts and all(p.shape == gt.shape and np.array_equal(p, gt) for p, gt in zip(preds, gts)):\n        correct += 1\nprint(f\"Accuracy: {correct}/{len(eval_ds)} = {correct/len(eval_ds):.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:32:12.940362Z","iopub.execute_input":"2025-07-17T19:32:12.940815Z","iopub.status.idle":"2025-07-17T19:37:19.786939Z","shell.execute_reply.started":"2025-07-17T19:32:12.940788Z","shell.execute_reply":"2025-07-17T19:37:19.786272Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nAccuracy: 0/120 = 0.00%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Training & Validasi","metadata":{}},{"cell_type":"markdown","source":"###### ─── 3) Training & Validasi (Revised) ───────────────────────────────────────\nimport os, json, numpy as np, pandas as pd, torch\nimport torch.nn.functional as F\nfrom numpy import ascontiguousarray\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom tqdm.auto import tqdm\n# Pastikan definisi dan imports sebelumnya:\n# ArcDataset, pad_grids, PerceptionEncoder, synthesize_program, NeuroSymbolicSolver\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\n# 1) Muat data training + solutions\nDATA_DIR    = '/kaggle/input/arc-prize-2025'\nTRAIN_CHALL = os.path.join(DATA_DIR, 'arc-agi_training_challenges.json')\nTRAIN_SOL   = os.path.join(DATA_DIR, 'arc-agi_training_solutions.json')\nfull_ds     = ArcDataset(TRAIN_CHALL, TRAIN_SOL, split='train')\nsamples     = [full_ds[i] for i in range(len(full_ds))]\nprint(f\"Total tasks: {len(samples)}\")\n\n# 2) Split 80/20\ntrain_samples, val_samples = train_test_split(samples, test_size=0.2, random_state=42)\nprint(f\"Train tasks: {len(train_samples)}, Val tasks: {len(val_samples)}\")\n\n# 3) Muat encoder terlatih\nencoder_base = PerceptionEncoder(num_colors=11, color_emb_dim=16, hidden_dim=128).to(device)\nencoder_base.load_state_dict(torch.load('perception_encoder_trained.pth', map_location=device))\nencoder_base.eval()\n\n# 3a) Utility: embed_grid dengan salinan kontigu tanpa stride negatif\ndef embed_grid(grid: np.ndarray) -> torch.Tensor:\n    # Buat salinan untuk menghapus negatif stride dan pastikan kontigu\n    arr = np.array(grid, copy=True)\n    arr = ascontiguousarray(arr)\n    t   = torch.from_numpy(arr).long().unsqueeze(0).to(device)\n    with torch.no_grad():\n        e = encoder_base(t)\n    return F.normalize(e.squeeze(0), dim=-1)\n\n# 4) Helper evaluasi solver\ndef evaluate_solver(solver, samples):\n    correct = 0\n    for sample in samples:\n        preds, _ = solver.solve(sample['train'], sample['test'])\n        gts = sample.get('test_gt', [])\n        if gts and all(p.shape == gt.shape and np.array_equal(p, gt)\n                       for p, gt in zip(preds, gts)):\n            correct += 1\n    return correct, len(samples)\n\n# 5) Grid search hyperparameter\ndef run_grid_search():\n    param_grid = {'beam_width': [5, 10, 20], 'max_depth': [3, 5]}\n    alpha      = 0.7  # bobot gabungan pixel & neural\n    records    = []\n    for params in tqdm(ParameterGrid(param_grid), desc='Grid Search'):\n        solver = NeuroSymbolicSolver(\n            encoder=encoder_base,\n            beam_width=params['beam_width'],\n            max_depth=params['max_depth'],\n            alpha=alpha\n        )\n        train_corr, train_tot = evaluate_solver(solver, train_samples)\n        val_corr,   val_tot   = evaluate_solver(solver, val_samples)\n        records.append({\n            'beam_width': params['beam_width'],\n            'max_depth' : params['max_depth'],\n            'train_acc' : train_corr/train_tot,\n            'val_acc'   : val_corr/val_tot\n        })\n    df = pd.DataFrame(records)\n    df = df.sort_values('val_acc', ascending=False).reset_index(drop=True)\n    print(df.to_string(index=False))\n    best = df.iloc[0]\n    print(f\"\\nBest config: beam_width={best.beam_width}, max_depth={best.max_depth}, alpha={alpha}\")\n    print(f\"Train Acc: {best.train_acc:.2%}, Val Acc: {best.val_acc:.2%}\")\n    df.to_csv('grid_search_results.csv', index=False)\n    print(\"Hasil grid search disimpan di grid_search_results.csv\")\n\n# 6) Jalankan grid search\nrun_grid_search()","metadata":{"execution":{"iopub.status.busy":"2025-07-17T19:45:10.825923Z","iopub.execute_input":"2025-07-17T19:45:10.826256Z"}}},{"cell_type":"markdown","source":"# Profiling Biaya & Kecepatan","metadata":{}},{"cell_type":"code","source":"import os, json, numpy as np, time, pandas as pd, torch\nfrom tqdm.auto import tqdm\n\n# 1) Muat evaluation set langsung\nDATA_DIR = '/kaggle/input/arc-prize-2025'\neval_ds = ArcDataset(\n    os.path.join(DATA_DIR, 'arc-agi_evaluation_challenges.json'),\n    os.path.join(DATA_DIR, 'arc-agi_evaluation_solutions.json'),\n    split='eval'\n)\n\n# 2) Solver dengan hyperparameter terbaik\nbest_beam, best_depth, alpha = 20, 5, 0.7\nencoder = PerceptionEncoder(num_colors=11, color_emb_dim=16, hidden_dim=128).to(device)\nencoder.load_state_dict(torch.load('perception_encoder_trained.pth', map_location=device))\nencoder.eval()\nsolver  = NeuroSymbolicSolver(encoder, beam_width=best_beam, max_depth=best_depth, alpha=alpha)\n\n# 3) Profiling per-task\nrecords = []\nfor sample in tqdm(eval_ds, desc=\"Profiling tasks\"):\n    tid         = sample['id']\n    train_pairs = sample['train']      # list of (inp: np.ndarray, out: np.ndarray)\n    test_inputs = sample['test']       # list of np.ndarray\n\n    # Warm-up\n    _ = solver.solve(train_pairs, test_inputs)\n\n    # Waktu total solve()\n    t0 = time.perf_counter()\n    preds, prog = solver.solve(train_pairs, test_inputs)\n    dt_total = time.perf_counter() - t0\n\n    # (Opsional) Ukur waktu scoring neural untuk satu contoh\n    # t1 = time.perf_counter()\n    # _ = embed_grid(train_pairs[0][0])\n    # dt_embed = time.perf_counter() - t1\n\n    # Hitung jumlah prakiraan kandidat per depth\n    num_base  = len(BASE_OPS)\n    num_fill  = len(set().union(*(np.unique(inp) for inp,_ in train_pairs)))\n    num_shift = 4  # kita pakai dx,dy in {(-1,1)} => 4 shift ops per program\n    ops_per_node = num_base + num_fill + num_shift\n    est_candidates = best_beam * ops_per_node * best_depth\n\n    records.append({\n        'task_id'      : tid,\n        'time_solve_s' : dt_total,\n        # 'time_embed_s': dt_embed,\n        'est_candidates': est_candidates,\n        'prog_len'     : len(prog.ops)\n    })\n\n# 4) Analisis hasil\ndf = pd.DataFrame(records)\nprint(f\"Total tasks profiled: {len(df)}\")\nprint(f\"Avg time/solve:   {df['time_solve_s'].mean():.3f}s\")\nprint(f\"Median time/solve:{df['time_solve_s'].median():.3f}s\")\nprint(f\"Max  time/solve:  {df['time_solve_s'].max():.3f}s\")\nprint(f\"Min  time/solve:  {df['time_solve_s'].min():.3f}s\")\n\nprint(\"\\n5 Slowest Tasks:\")\nprint(df.nlargest(5, 'time_solve_s')[['task_id','time_solve_s','prog_len','est_candidates']])\n\n# 5) Simpan profiling\ndf.to_csv('profiling_results.csv', index=False)\nprint(\"\\nSaved profiling_results.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T19:48:16.404913Z","iopub.execute_input":"2025-07-17T19:48:16.405222Z","iopub.status.idle":"2025-07-17T19:58:55.994310Z","shell.execute_reply.started":"2025-07-17T19:48:16.405203Z","shell.execute_reply":"2025-07-17T19:58:55.993640Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Profiling tasks:   0%|          | 0/120 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d558d9c1a7b842a49af5177a3423b8e4"}},"metadata":{}},{"name":"stdout","text":"Total tasks profiled: 120\nAvg time/solve:   2.665s\nMedian time/solve:2.602s\nMax  time/solve:  5.143s\nMin  time/solve:  1.610s\n\n5 Slowest Tasks:\n      task_id  time_solve_s  prog_len  est_candidates\n101  d8e07eb2      5.143372         0            1900\n26   36a08778      5.009499         0            1300\n66   88bcf3b4      4.590005         0            2000\n104  db695cfb      4.464399         0            1500\n91   b6f77b65      4.398156         0            2000\n\nSaved profiling_results.csv\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Tuning & Iterasi","metadata":{}},{"cell_type":"code","source":"# ===============================\n# 6) TUNING & ITERASI (Revised)\n# ===============================\n\nimport os, json, numpy as np, pandas as pd, torch\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom tqdm.auto import tqdm\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\n# 0) Pastikan definisi dan imports:\n#    ArcDataset, pad_grids, PerceptionEncoder, embed_grid,\n#    synthesize_program, NeuroSymbolicSolver\n\n# 1) Muat & split data\nDATA_DIR    = '/kaggle/input/arc-prize-2025'\nTRAIN_CHALL = os.path.join(DATA_DIR, 'arc-agi_training_challenges.json')\nTRAIN_SOL   = os.path.join(DATA_DIR, 'arc-agi_training_solutions.json')\nfull_ds     = ArcDataset(TRAIN_CHALL, TRAIN_SOL, split='train')\nall_samples = [full_ds[i] for i in range(len(full_ds))]\ntrain_samples, val_samples = train_test_split(all_samples, test_size=0.2, random_state=42)\nprint(f\"Train tasks: {len(train_samples)}, Val tasks: {len(val_samples)}\")\n\n# 2) Load encoder terlatih sekali saja\nencoder_base = PerceptionEncoder(num_colors=11, color_emb_dim=16, hidden_dim=128).to(device)\nencoder_base.load_state_dict(torch.load('perception_encoder_trained.pth', map_location=device))\nencoder_base.eval()\n\n# (Re)def embed_grid jika perlu, untuk scoring neural\nimport torch.nn.functional as F\nfrom numpy import ascontiguousarray\ndef embed_grid(grid: np.ndarray) -> torch.Tensor:\n    arr = ascontiguousarray(np.array(grid, copy=True))\n    t   = torch.from_numpy(arr).long().unsqueeze(0).to(device)\n    with torch.no_grad():\n        e = encoder_base(t)\n    return F.normalize(e.squeeze(0), dim=-1)\n\n# 3) Helper evaluasi\ndef evaluate_solver(solver, samples):\n    correct = 0\n    for sample in samples:\n        preds, _ = solver.solve(sample['train'], sample['test'])\n        gts = sample.get('test_gt', [])\n        if gts and all(p.shape==gt.shape and np.array_equal(p,gt) for p,gt in zip(preds,gts)):\n            correct += 1\n    return correct, len(samples)\n\n# 4) Grid search hyperparameter (solver-only)\nparam_grid = {\n    'beam_width': [5, 10, 20],\n    'max_depth' : [3, 5],\n    'alpha'     : [0.5, 0.7, 0.9]\n}\n\nrecords = []\nfor params in tqdm(ParameterGrid(param_grid), desc='Tuning & Iterasi'):\n    solver = NeuroSymbolicSolver(\n        encoder=encoder_base,\n        beam_width=params['beam_width'],\n        max_depth=params['max_depth'],\n        alpha=params['alpha']\n    )\n    train_corr, train_tot = evaluate_solver(solver, train_samples)\n    val_corr,   val_tot   = evaluate_solver(solver, val_samples)\n    records.append({\n        **params,\n        'train_acc': train_corr/train_tot,\n        'val_acc'  : val_corr/val_tot\n    })\n    print(f\"  → bw={params['beam_width']}, md={params['max_depth']}, α={params['alpha']} \"\n          f\"=> train={train_corr/train_tot:.2%}, val={val_corr/val_tot:.2%}\")\n\n# 5) Ringkasan & pilih konfigurasi terbaik\ndf = pd.DataFrame(records).sort_values('val_acc', ascending=False).reset_index(drop=True)\nprint(\"\\nTop 5 konfigurasi menurut val_acc:\")\nprint(df.head(5).to_string(index=False))\n\nbest = df.iloc[0]\nprint(f\"\\nBest config: beam_width={best.beam_width}, max_depth={best.max_depth}, alpha={best.alpha}\")\nprint(f\"Val Acc: {best.val_acc:.2%}\")\n\n# 6) Simpan hasil tuning\ndf.to_csv('tuning_iterations.csv', index=False)\nprint(\"Hasil tuning disimpan ke tuning_iterations.csv\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluasi Akhir & Submission","metadata":{}},{"cell_type":"code","source":"# ─── 7) Final Evaluation & Submission (Revised) ──────────────────────────────\n\nimport os\nimport json\nimport numpy as np\nimport torch\nfrom tqdm.auto import tqdm\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\n# --- 0) Hyperparameters & paths (ganti sesuai hasil tuning!) ---\nbest_beam   = 20      # contoh: hasil tuning\nbest_depth  = 5       # contoh: hasil tuning\nbest_alpha  = 0.7     # contoh: hasil tuning\ncolor_emb   = 16      # harus sama dengan training encoder\nhidden_dim  = 128     # harus sama dengan training encoder\n\nDATA_DIR        = '/kaggle/input/arc-prize-2025'\neval_chal_path  = os.path.join(DATA_DIR, 'arc-agi_evaluation_challenges.json')\neval_sol_path   = os.path.join(DATA_DIR, 'arc-agi_evaluation_solutions.json')\ntest_chal_path  = os.path.join(DATA_DIR, 'arc-agi_test_challenges.json')\n\n# --- 1) Load evaluation set (with ground truth) ---\neval_ds = ArcDataset(eval_chal_path, eval_sol_path, split='eval')\n\n# --- 2) Load pretrained encoder and instantiate solver ---\nencoder = PerceptionEncoder(\n    num_colors=11,\n    color_emb_dim=color_emb,\n    hidden_dim=hidden_dim\n).to(device)\nencoder.load_state_dict(torch.load('perception_encoder_trained.pth', map_location=device))\nencoder.eval()\n\nsolver = NeuroSymbolicSolver(\n    encoder=encoder,\n    beam_width=best_beam,\n    max_depth=best_depth,\n    alpha=best_alpha\n)\n\n# --- 3) Final evaluation on the public eval set ---\ncorrect = 0\ntotal   = 0\nfor sample in tqdm(eval_ds, desc='Final Evaluation'):\n    preds, _ = solver.solve(sample['train'], sample['test'])\n    gts      = sample.get('test_gt', [])\n    if gts and all(p.shape == gt.shape and np.array_equal(p, gt)\n                   for p,gt in zip(preds, gts)):\n        correct += 1\n    total += 1\n\nprint(f'Final Eval Accuracy: {correct}/{total} = {correct/total:.2%}')\n\n# --- 4) Generate submission on the test set (no solutions file) ---\ntest_ds = ArcDataset(test_chal_path, solutions_path=None, split='test')\n\nsubmission = {}\nfor sample in tqdm(test_ds, desc='Generating Submission'):\n    tid   = sample['id']\n    preds, _ = solver.solve(sample['train'], sample['test'])\n    # buat dua attempts identik per input\n    submission[tid] = [\n        {\"attempt_1\": p.tolist(), \"attempt_2\": p.tolist()}\n        for p in preds\n    ]\n\n# --- 5) Write submission.json ---\nwith open('submission.json', 'w') as f:\n    json.dump(submission, f)\nprint(f'\\nSubmission written: submission.json ({len(submission)} tasks).')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T04:55:08.138763Z","iopub.execute_input":"2025-07-17T04:55:08.139069Z","iopub.status.idle":"2025-07-17T04:55:12.426830Z","shell.execute_reply.started":"2025-07-17T04:55:08.139029Z","shell.execute_reply":"2025-07-17T04:55:12.426005Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Final Evaluation:   0%|          | 0/120 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b84c28a8c0e4211a6b5571cd1f568df"}},"metadata":{}},{"name":"stdout","text":"Final Eval Accuracy: 0/120 = 0.00%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating Submission:   0%|          | 0/240 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5387ab6f6d3847a19d5491de287b9629"}},"metadata":{}},{"name":"stdout","text":"submission.json written (240 tasks).\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# ─── 7) Final Evaluation & Submission (Revised with Sanity Checks) ─────────────\n\nimport os\nimport json\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\n# 1) Instantiate solver with best hyperparameters and load pretrained encoder\nbest_beam   = 20      # ← ganti sesuai hasil tuning\nbest_depth  = 5       # ← ganti sesuai hasil tuning\nbest_alpha  = 0.7     # ← ganti sesuai hasil tuning\ncolor_emb   = 16      # harus sama dengan saat training encoder\nhidden_dim  = 128     # harus sama dengan saat training encoder\n\nencoder = PerceptionEncoder(\n    num_colors=11,\n    color_emb_dim=color_emb,\n    hidden_dim=hidden_dim\n).to(device)\nencoder.load_state_dict(torch.load('perception_encoder_trained.pth', map_location=device))\nencoder.eval()\n\nsolver = NeuroSymbolicSolver(\n    encoder=encoder,\n    beam_width=best_beam,\n    max_depth=best_depth,\n    alpha=best_alpha\n)\n\n# 2) Load test set via ArcDataset\nDATA_DIR    = '/kaggle/input/arc-prize-2025'\ntest_path   = os.path.join(DATA_DIR, 'arc-agi_test_challenges.json')\ntest_ds     = ArcDataset(test_path, solutions_path=None, split='test')\ntest_loader = DataLoader(test_ds, batch_size=1, shuffle=False, collate_fn=collate_fn)\n\n# 3) Build submission dictionary\nsubmission = {}\nfor batched, _ in tqdm(test_loader, desc='Building Submission'):\n    tid         = batched['id'][0]\n    train_pairs = list(zip(batched['train_in'], batched['train_out']))\n    test_inputs = batched['test_in']\n    preds, _    = solver.solve(train_pairs, test_inputs)\n\n    entries = []\n    for inp, pred in zip(test_inputs, preds):\n        # Validate shape: fallback to blank grid if mismatch\n        if pred.shape != inp.shape:\n            pred = np.zeros_like(inp)\n        arr = pred.astype(int).tolist()\n        entries.append({'attempt_1': arr, 'attempt_2': arr})\n    submission[tid] = entries\n\n# 4) Sanity checks\n# 4a) All task IDs present\ntask_ids = {rec['id'] for rec in test_ds.challenges}\nmissing  = task_ids - set(submission.keys())\nassert not missing, f\"Missing {len(missing)} tasks: {missing}\"\n\n# 4b) Correct number of outputs per task\nfor rec in test_ds.challenges:\n    tid      = rec['id']\n    expected = len(rec['test'])\n    actual   = len(submission[tid])\n    assert actual == expected, f\"Task {tid}: expected {expected} outputs, got {actual}\"\n\n# 5) Save submission\nwith open('submission.json', 'w') as f:\n    json.dump(submission, f)\nprint(f\"\\nsubmission.json written with {len(submission)} tasks.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:08:44.099649Z","iopub.execute_input":"2025-07-17T05:08:44.099975Z","iopub.status.idle":"2025-07-17T05:08:47.166195Z","shell.execute_reply.started":"2025-07-17T05:08:44.099952Z","shell.execute_reply":"2025-07-17T05:08:47.165311Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Building Submission:   0%|          | 0/240 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14142f361a9542f0acf4c1d14f8db0f2"}},"metadata":{}},{"name":"stdout","text":"submission.json written with 240 tasks.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"# ─── Validate submission.json ────────────────────────────────────────────────\n\nimport json\nimport os\nfrom torch.utils.data import DataLoader\n\n# 1) Pastikan file ada\nif not os.path.exists('submission.json'):\n    raise FileNotFoundError(\"submission.json not found in current directory.\")\nprint(\"Found submission.json\")\n\n# 2) Muat submission\nwith open('submission.json') as f:\n    submission = json.load(f)\nassert isinstance(submission, dict), \"submission.json harus berisi dict\"\n\n# 3) Load test challenges to check counts\nDATA_DIR   = '/kaggle/input/arc-prize-2025'\ntest_path  = os.path.join(DATA_DIR, 'arc-agi_test_challenges.json')\ntest_ds    = ArcDataset(test_path, solutions_path=None, split='test')\n\n# 4) Per-task checks\nfor rec in test_ds.challenges:\n    tid      = rec['id']\n    assert tid in submission, f\"Task ID {tid} missing in submission\"\n    outputs  = submission[tid]\n    # Shape check: must be list of dicts with 2 keys\n    assert isinstance(outputs, list), f\"Outputs for {tid} must be a list\"\n    expected = len(rec['test'])\n    actual   = len(outputs)\n    assert actual == expected, f\"Task {tid}: expected {expected} entries, got {actual}\"\n    for entry in outputs:\n        assert set(entry.keys()) == {'attempt_1','attempt_2'}, (\n            f\"Each entry for {tid} must have exactly 'attempt_1' and 'attempt_2' keys\"\n        )\n        # Ensure the arrays have correct shape\n        # We can check list lengths match input shape\n        inp_shape = np.array(rec['test'][0]).shape\n        for key in ('attempt_1','attempt_2'):\n            arr = np.array(entry[key])\n            assert arr.shape == inp_shape, (\n                f\"{key} for task {tid} has shape {arr.shape}, expected {inp_shape}\"\n            )\n\nprint(\"\\nsubmission.json format and contents look good!\\n\")\n\n# 5) Display first 5 entries for manual inspection\nprint(\"=== First 5 submission entries ===\")\nfor idx, tid in enumerate(list(submission.keys())[:5]):\n    outputs = submission[tid]\n    print(f\"\\nTask ID: {tid} (n_outputs={len(outputs)})\")\n    for j, entry in enumerate(outputs):\n        print(f\"  Output #{j+1}:\")\n        print(\"    attempt_1:\", entry['attempt_1'])\n        print(\"    attempt_2:\", entry['attempt_2'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T05:12:23.272062Z","iopub.execute_input":"2025-07-17T05:12:23.272588Z","iopub.status.idle":"2025-07-17T05:12:23.292388Z","shell.execute_reply.started":"2025-07-17T05:12:23.272564Z","shell.execute_reply":"2025-07-17T05:12:23.291795Z"}},"outputs":[{"name":"stdout","text":"Submission.json format looks good!\n\n=== First 5 submission entries ===\n\nTask ID: 00576224\n  Output #1:\n    attempt_1: [[3, 2], [7, 8]]\n    attempt_2: [[3, 2], [7, 8]]\n\nTask ID: 007bbfb7\n  Output #1:\n    attempt_1: [[7, 0, 7], [7, 0, 7], [7, 7, 0]]\n    attempt_2: [[7, 0, 7], [7, 0, 7], [7, 7, 0]]\n\nTask ID: 009d5c81\n  Output #1:\n    attempt_1: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8], [0, 0, 0, 0, 0, 8, 0, 0, 0, 8, 0, 8, 0, 8], [0, 0, 0, 0, 0, 8, 0, 8, 0, 8, 0, 0, 0, 8], [0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n    attempt_2: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8], [0, 0, 0, 0, 0, 8, 0, 0, 0, 8, 0, 8, 0, 8], [0, 0, 0, 0, 0, 8, 0, 8, 0, 8, 0, 0, 0, 8], [0, 0, 0, 0, 0, 8, 8, 8, 8, 8, 8, 8, 8, 8], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n\nTask ID: 00d62c1b\n  Output #1:\n    attempt_1: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 3, 0, 3, 3, 3, 3, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0], [0, 0, 3, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 3, 0, 3, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n    attempt_2: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 3, 0, 3, 3, 3, 3, 3, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0], [0, 0, 3, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 3, 0, 3, 0, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n\nTask ID: 00dbd492\n  Output #1:\n    attempt_1: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0], [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0], [0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0], [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0], [0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n    attempt_2: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0], [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0], [0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0], [0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0], [0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0], [0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"**Dokumentasi & Paper Draft**","metadata":{}}]}