{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:31:21.140722Z","iopub.execute_input":"2025-07-19T15:31:21.141186Z","iopub.status.idle":"2025-07-19T15:31:21.148412Z","shell.execute_reply.started":"2025-07-19T15:31:21.141162Z","shell.execute_reply":"2025-07-19T15:31:21.147793Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/arc-prize-2025/arc-agi_training_solutions.json\n/kaggle/input/arc-prize-2025/arc-agi_evaluation_solutions.json\n/kaggle/input/arc-prize-2025/arc-agi_evaluation_challenges.json\n/kaggle/input/arc-prize-2025/sample_submission.json\n/kaggle/input/arc-prize-2025/arc-agi_training_challenges.json\n/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"**Setup & Instalasi**","metadata":{}},{"cell_type":"code","source":"!pip install networkx tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:31:21.161092Z","iopub.execute_input":"2025-07-19T15:31:21.161336Z","iopub.status.idle":"2025-07-19T15:31:24.428386Z","shell.execute_reply.started":"2025-07-19T15:31:21.161319Z","shell.execute_reply":"2025-07-19T15:31:24.427671Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.5)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# 1) Import library dasar\nimport os\nimport json\nimport numpy as np\nimport torch\nimport random\n\n# 2) Pastikan reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# 3) Cek perangkat (CPU/GPU)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using device: {device}')\n\n# 4) Path data\nDATA_DIR = '/kaggle/input/arc-prize-2025'\n\n# 5) (Opsional) Install dependensi tambahan\n#    Kaggle image container sudah menyertakan PyTorch, NumPy, dll.\n#    Jika butuh library lain, uncomment dan sesuaikan:\n\n\n# 6) Buat direktori kerja untuk output/model\nWORK_DIR = '/kaggle/working'\nos.makedirs(WORK_DIR, exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:31:24.430514Z","iopub.execute_input":"2025-07-19T15:31:24.430823Z","iopub.status.idle":"2025-07-19T15:31:24.444139Z","shell.execute_reply.started":"2025-07-19T15:31:24.430791Z","shell.execute_reply":"2025-07-19T15:31:24.443451Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"# Eksplorasi Data (EDA) ","metadata":{}},{"cell_type":"code","source":"# ─── EDA Lengkap untuk ARC Prize 2025 ────────────────────────────────────────\n\nimport os, json, glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter, defaultdict\nfrom scipy.ndimage import label\nimport scipy.stats as st\n\n# 1) Temukan dan load file training tasks\n# Sesuaikan path jika perlu\nTRAIN_PATH = '/kaggle/input/arc-prize-2025/arc-agi_training_challenges.json'\nassert os.path.exists(TRAIN_PATH), f\"File not found: {TRAIN_PATH}\"\nwith open(TRAIN_PATH) as f:\n    raw = json.load(f)\ntasks = raw.get('tasks') or list(raw.values())\nprint(f\"Loaded {len(tasks)} tasks from {TRAIN_PATH}\")\n\n# 2) Definisikan detektor transform dasar\ndef identity(x):    return x\ndef rot90(x):       return np.rot90(x,1)\ndef rot180(x):      return np.rot90(x,2)\ndef rot270(x):      return np.rot90(x,3)\ndef flip_h(x):      return np.fliplr(x)\ndef flip_v(x):      return np.flipud(x)\ndef shift(x,dx,dy): return np.roll(np.roll(x,dy,axis=0),dx,axis=1)\n\nBASIC_OPS = {\n    'identity': identity,\n    'rot90':    rot90,\n    'rot180':   rot180,\n    'rot270':   rot270,\n    'flip_h':   flip_h,\n    'flip_v':   flip_v\n}\n\ndef detect_basic(inp, out):\n    # coba rotasi/flip\n    for name, fn in BASIC_OPS.items():\n        o2 = fn(inp)\n        if o2.shape==out.shape and np.array_equal(o2,out):\n            return name\n    # coba shift ±1px\n    for dx in (-1,1):\n        for dy in (-1,1):\n            s = shift(inp,dx,dy)\n            if s.shape==out.shape and np.array_equal(s,out):\n                return f'shift({dx},{dy})'\n    # coba fill sekali background→warna baru\n    vals,counts = np.unique(inp, return_counts=True)\n    bg = vals[np.argmax(counts)]\n    for c in np.unique(out):\n        if c!=bg:\n            tmp = inp.copy()\n            tmp[inp==bg] = c\n            if tmp.shape==out.shape and np.array_equal(tmp,out):\n                return f'fill({c})'\n    return 'other'\n\n# 3) Kumpulkan statistik\nbasic_freq  = Counter()\ncolor_map   = Counter()\nbbox_diffs  = []\nregion_counts = []\n\nfor t in tasks:\n    for ex in t['train']:\n        inp = np.array(ex['input'])\n        out = np.array(ex['output'])\n        # 3a) basic ops\n        op = detect_basic(inp, out)\n        basic_freq[op] += 1\n\n        # 3b) color mapping & bbox hanya kalau shapes sama\n        if inp.shape == out.shape:\n            mask = (inp != out)\n            for a,b in zip(inp[mask].flatten(), out[mask].flatten()):\n                color_map[(a,b)] += 1\n\n            # region count\n            lab_i, ni = label(np.ones_like(inp))  # full grid\n            lab_o, no = label(np.ones_like(out))\n            region_counts.append((ni, no))\n\n            # bbox shift per warna\n            common = set(np.unique(inp)).intersection(out.flat)\n            for c in common:\n                ys_i, xs_i = np.where(inp==c)\n                ys_o, xs_o = np.where(out==c)\n                if ys_i.size and ys_o.size:\n                    dx = xs_o.mean() - xs_i.mean()\n                    dy = ys_o.mean() - ys_i.mean()\n                    bbox_diffs.append((dx, dy))\n\n        else:\n            region_counts.append((None, None))\n\n# 4) Ringkasan statistik\n\n# 4a) Basic ops\ndf_basic = pd.DataFrame.from_dict(basic_freq, orient='index', columns=['count'])\ndf_basic = df_basic.sort_values('count', ascending=False)\nprint(\"=== Frequent Basic Ops ===\")\nprint(df_basic)\n\n# 4b) Top color mappings\ntop_maps = color_map.most_common(10)\nprint(\"\\n=== Top-10 Color Mappings (old→new) ===\")\nprint(pd.DataFrame(top_maps, columns=['(old→new)','count']))\n\n# 4c) Region count changes\ndf_reg = pd.DataFrame(region_counts, columns=['in_regions','out_regions'])\nprint(\"\\n=== Region Counts Description ===\")\nprint(df_reg.dropna().describe())\n\n# 4d) Bounding-box shifts\ndf_bbox = pd.DataFrame(bbox_diffs, columns=['dx','dy'])\nprint(\"\\n=== Bounding-Box Shift Stats ===\")\nprint(df_bbox.describe().loc[['mean','std']])\n\n# 4e) Entropy of input color distributions\nentropies = []\nfor t in tasks:\n    inp = np.array(t['train'][0]['input'])\n    freqs = np.bincount(inp.flatten())\n    entropies.append(st.entropy(freqs[freqs>0]))\ndf_ent = pd.Series(entropies, name='entropy')\nprint(\"\\n=== Input-Grid Color Entropy ===\")\nprint(df_ent.describe())\n\n# 5) Visualisasi sederhana\nplt.figure(figsize=(6,3)); df_basic['count'].plot.bar(); plt.title(\"Basic Ops Distribusi\"); plt.show()\n\n# 6) Rekomendasi berikutnya\nprint(\"\\n=== Rekomendasi Primitives Baru ===\")\nif df_basic.get('other',0) / df_basic['count'].sum() > 0.3:\n    print(\"→ Banyak 'other' → tambahkan primitives: map_colors, extract_region, overlay\")\nif top_maps and top_maps[0][1] > 50:\n    print(f\"→ Pemetaan warna umum: {top_maps[:3]} → tambahkan map_colors(mapping_dict)\")\nif df_bbox['dx'].abs().mean() > 1 or df_bbox['dy'].abs().mean() > 1:\n    print(\"→ Rata² region berpindah >1px → tambahkan move_region(color, dx, dy)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:31:24.444864Z","iopub.execute_input":"2025-07-19T15:31:24.445053Z","iopub.status.idle":"2025-07-19T15:31:27.081827Z","shell.execute_reply.started":"2025-07-19T15:31:24.445039Z","shell.execute_reply":"2025-07-19T15:31:27.081036Z"}},"outputs":[{"name":"stdout","text":"Loaded 1000 tasks from /kaggle/input/arc-prize-2025/arc-agi_training_challenges.json\n=== Frequent Basic Ops ===\n             count\nother         3196\nidentity        10\nrot180           8\nflip_h           5\nflip_v           4\nrot90            4\nshift(-1,1)      2\nfill(5)          2\nfill(2)          1\n\n=== Top-10 Color Mappings (old→new) ===\n  (old→new)  count\n0    (0, 3)   9558\n1    (0, 2)   7341\n2    (0, 8)   5794\n3    (0, 4)   4890\n4    (0, 1)   4524\n5    (0, 7)   2179\n6    (0, 6)   1851\n7    (0, 5)   1541\n8    (1, 2)   1123\n9    (3, 0)   1103\n\n=== Region Counts Description ===\n       in_regions  out_regions\ncount      2114.0       2114.0\nmean          1.0          1.0\nstd           0.0          0.0\nmin           1.0          1.0\n25%           1.0          1.0\n50%           1.0          1.0\n75%           1.0          1.0\nmax           1.0          1.0\n\n=== Bounding-Box Shift Stats ===\n            dx        dy\nmean  0.241136  0.178076\nstd   1.979561  1.984115\n\n=== Input-Grid Color Entropy ===\ncount    1000.000000\nmean        0.724911\nstd         0.414405\nmin         0.000000\n25%         0.431750\n50%         0.666894\n75%         0.955349\nmax         2.223468\nName: entropy, dtype: float64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x300 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\n=== Rekomendasi Primitives Baru ===\n→ Pemetaan warna umum: [((0, 3), 9558), ((0, 2), 7341), ((0, 8), 5794)] → tambahkan map_colors(mapping_dict)\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# ─── Tambah map_colors ke primitives ────────────────────────────────────────\n\ndef map_colors(x, mapping):\n    \"\"\"\n    mapping: dict old_color->new_color\n    contoh: {0:3} akan mengganti semua 0→3\n    \"\"\"\n    y = x.copy()\n    for old, new in mapping.items():\n        y[x == old] = new\n    return y\n\n# Di awal, setelah BASIC_OPS:\nEXTENDED_OPS = dict(BASIC_OPS)  # copy dasar\n# Berdasar EDA:\ntop_maps = [(0,3),(0,2),(0,8),(0,4),(0,1)]\nfor old,new in top_maps:\n    EXTENDED_OPS[f\"map_colors({old}->{new})\"] = lambda x, o=old, n=new: map_colors(x,{o:n})\n\n# ─── Dalam synthesize_program, ganti penggunaan BASE_OPS dengan EXTENDED_OPS ─\ndef synthesize_program(pairs, beam_width, max_depth, alpha, encoder, device):\n    beam = [Program()]\n    best, best_sc = beam[0], score_program(beam[0], pairs, encoder, alpha, device)\n\n    # gunakan EXTENDED_OPS di sini\n    for _ in range(max_depth):\n        cands = []\n        for prog in beam:\n            # coba semua extended ops\n            for name, fn in EXTENDED_OPS.items():\n                p2 = prog.extend(name, fn)\n                sc = score_program(p2, pairs, encoder, alpha, device)\n                cands.append((sc, p2))\n                if sc > best_sc:\n                    best_sc, best = sc, p2\n        # prune\n        cands.sort(key=lambda x: x[0], reverse=True)\n        beam = [p for _, p in cands[:beam_width]]\n    return best","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:31:27.083591Z","iopub.execute_input":"2025-07-19T15:31:27.083818Z","iopub.status.idle":"2025-07-19T15:31:27.091592Z","shell.execute_reply.started":"2025-07-19T15:31:27.083799Z","shell.execute_reply":"2025-07-19T15:31:27.090941Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"# Baseline Heuristik","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nfrom collections import Counter\n\n# ─── Robust ArcDataset loader ────────────────────────────────────────────────\nclass ArcDataset:\n    def __init__(self, challenges_path, solutions_path=None, split='train'):\n        raw = json.load(open(challenges_path))\n        if isinstance(raw, dict) and 'tasks' in raw:\n            tasks = raw['tasks']\n        elif isinstance(raw, dict) and all(isinstance(v, dict) for v in raw.values()):\n            tasks = []\n            for tid, rec in raw.items():\n                rec = rec.copy(); rec['id'] = tid\n                tasks.append(rec)\n        else:\n            tasks = raw\n        self.challenges = tasks\n\n        self.solutions = {}\n        if solutions_path and os.path.exists(solutions_path):\n            raw_s = json.load(open(solutions_path))\n            sol_list = raw_s.get('tasks') if isinstance(raw_s, dict) and 'tasks' in raw_s else (\n                       raw_s if isinstance(raw_s, list) else list(raw_s.values()))\n            # flatten nested lists\n            flat = []\n            for rec in sol_list:\n                if isinstance(rec, dict):\n                    flat.append(rec)\n                elif isinstance(rec, list):\n                    flat.extend([r for r in rec if isinstance(r, dict)])\n            for rec in flat:\n                tid = rec.get('id') or rec.get('task_id')\n                if tid is not None:\n                    self.solutions[tid] = rec\n        self.split = split\n\n    def __len__(self):\n        return len(self.challenges)\n\n    def __getitem__(self, idx):\n        t = self.challenges[idx]\n        tid = t['id']\n        train_pairs = [(np.array(ex['input']), np.array(ex['output'])) for ex in t['train']]\n        test_inputs = [np.array(ex['input']) for ex in t['test']]\n        sample = {'id': tid, 'train': train_pairs, 'test': test_inputs}\n        if self.split in ('train','eval') and tid in self.solutions:\n            sol = self.solutions[tid]\n            if 'test' in sol:\n                sample['test_gt'] = [np.array(ex['output']) for ex in sol['test']]\n        return sample\n\n# ─── Baseline Heuristic: Best Transform + Color Mapping ─────────────────────\n# 1) Define candidate transformations\ndef identity(x):    return x.copy()\ndef rot90(x):       return np.rot90(x, 1)\ndef rot180(x):      return np.rot90(x, 2)\ndef rot270(x):      return np.rot90(x, 3)\ndef flip_h(x):      return np.fliplr(x)\ndef flip_v(x):      return np.flipud(x)\n\nTRANSFORMS = {\n    'identity': identity,\n    'rot90':    rot90,\n    'rot180':   rot180,\n    'rot270':   rot270,\n    'flip_h':   flip_h,\n    'flip_v':   flip_v,\n}\n\n# 2) Derive best transform + mapping for a task\ndef find_best_transform_mapping(train_pairs):\n    best = ('identity', {}); best_score = -1.0\n    for name, fn in TRANSFORMS.items():\n        # gather mapping counts\n        cnt = Counter()\n        total_pixels = 0\n        for inp, out in train_pairs:\n            t_inp = fn(inp)\n            if t_inp.shape != out.shape:\n                continue\n            mask = t_inp != out\n            cnt.update(zip(t_inp[mask].flatten(), out[mask].flatten()))\n            total_pixels += out.size\n        # build mapping\n        mapping = {}\n        by_old = {}\n        for (old, new), c in cnt.items():\n            if old not in by_old or c > by_old[old][1]:\n                by_old[old] = (new, c)\n        mapping = {old: new for old, (new, _) in by_old.items()}\n        # compute score\n        matched = 0\n        for inp, out in train_pairs:\n            t_inp = fn(inp)\n            if t_inp.shape != out.shape:\n                continue\n            pred = t_inp.copy()\n            for old, new in mapping.items():\n                pred[t_inp == old] = new\n            matched += np.sum(pred == out)\n        score = matched / total_pixels if total_pixels > 0 else 0\n        if score > best_score:\n            best_score = score\n            best = (name, mapping)\n    return best\n\n# 3) Apply baseline to evaluation set\ndef baseline_evaluate():\n    DATA_DIR = '/kaggle/input/arc-prize-2025'\n    eval_chal = os.path.join(DATA_DIR, 'arc-agi_evaluation_challenges.json')\n    eval_sol  = os.path.join(DATA_DIR, 'arc-agi_evaluation_solutions.json')\n    ds = ArcDataset(eval_chal, eval_sol, split='eval')\n\n    correct = 0\n    for sample in ds:\n        # find best transform & mapping from training\n        name, mapping = find_best_transform_mapping(sample['train'])\n        fn = TRANSFORMS[name]\n        # predict test\n        preds = []\n        for inp in sample['test']:\n            t_inp = fn(inp)\n            pred = t_inp.copy()\n            for old, new in mapping.items():\n                pred[t_inp == old] = new\n            preds.append(pred)\n        # evaluate\n        gts = sample.get('test_gt', [])\n        if gts and all(np.array_equal(p,gt) for p,gt in zip(preds, gts)):\n            correct += 1\n    total = len(ds)\n    print(f\"Baseline heuristic accuracy: {correct}/{total} = {correct/total:.2%}\")\n\n# 4) Run baseline evaluation\nbaseline_evaluate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:31:27.092506Z","iopub.execute_input":"2025-07-19T15:31:27.093008Z","iopub.status.idle":"2025-07-19T15:31:27.331068Z","shell.execute_reply.started":"2025-07-19T15:31:27.092989Z","shell.execute_reply":"2025-07-19T15:31:27.329827Z"}},"outputs":[{"name":"stdout","text":"Baseline heuristic accuracy: 0/120 = 0.00%\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"# Perception Encoder","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n\n# 1) Dataset ARC: hanya load challenge JSON, jaga agar setiap task punya 'id'\nclass ArcDataset(Dataset):\n    def __init__(self, challenges_path, split='train'):\n        raw = json.load(open(challenges_path))\n        # format {\"tasks\":[...]}\n        if isinstance(raw, dict) and 'tasks' in raw:\n            tasks = raw['tasks']\n        # format {\"task_id\": {...}, ...}\n        elif isinstance(raw, dict) and all(isinstance(v, dict) for v in raw.values()):\n            tasks = []\n            for tid, rec in raw.items():\n                rec2 = rec.copy()\n                rec2['id'] = tid\n                tasks.append(rec2)\n        # atau langsung list\n        else:\n            tasks = raw\n\n        # pastikan setiap record ada field 'id'\n        for rec in tasks:\n            if 'id' not in rec and 'task_id' in rec:\n                rec['id'] = rec['task_id']\n\n        self.tasks = tasks\n        self.split = split\n\n    def __len__(self):\n        return len(self.tasks)\n\n    def __getitem__(self, idx):\n        rec = self.tasks[idx]\n        tid = rec['id']\n        # ambil pasangan train input/output\n        train_pairs = [\n            (np.array(ex['input']), np.array(ex['output']))\n            for ex in rec['train']\n        ]\n        # ambil list test inputs\n        test_grids = [np.array(ex['input']) for ex in rec['test']]\n        return train_pairs, test_grids, tid\n\n# 2) Utility: pad list of 2D-arrays jadi tensor (B,Hmax,Wmax)\ndef pad_and_stack(grids, pad_value=0):\n    max_h = max(g.shape[0] for g in grids)\n    max_w = max(g.shape[1] for g in grids)\n    batch = np.full((len(grids), max_h, max_w), pad_value, dtype=np.int64)\n    for i, g in enumerate(grids):\n        h, w = g.shape\n        batch[i, :h, :w] = g\n    return torch.from_numpy(batch)\n\n# 3) collate_fn untuk DataLoader: kumpulkan semua train/input, train/output, plus test per batch\ndef collate_fn(batch):\n    all_in, all_out = [], []\n    ids, test_lists = [], []\n    for train_pairs, test_grids, tid in batch:\n        for inp, out in train_pairs:\n            all_in.append(inp)\n            all_out.append(out)\n        ids.append(tid)\n        test_lists.append(test_grids)\n    stacked_in  = pad_and_stack(all_in)\n    stacked_out = pad_and_stack(all_out)\n    return stacked_in, stacked_out, test_lists, ids\n\n# 4) Perception Encoder with coordinate channels\nclass PerceptionEncoderCoord(nn.Module):\n    def __init__(self, num_colors=11, color_emb_dim=16, hidden_dim=128):\n        super().__init__()\n        # embedding per-warna\n        self.color_emb = nn.Embedding(num_colors, color_emb_dim)\n        # tiga layer konvolusi, menambahkan 2 channel koordinat\n        self.conv1 = nn.Conv2d(color_emb_dim + 2, 64, 3, padding=1)\n        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n        self.conv3 = nn.Conv2d(128, hidden_dim, 3, padding=1)\n        self.pool  = nn.AdaptiveAvgPool2d((1,1))\n\n    def forward(self, grids):\n        # grids: LongTensor (B,H,W)\n        B, H, W = grids.shape\n        # embed warna → (B,H,W,C)\n        x = self.color_emb(grids).permute(0,3,1,2).contiguous()  # → (B,C,H,W)\n        # tambahkan coordinate channels (range [0,1])\n        xs = torch.linspace(0,1,W,device=grids.device).view(1,1,1,W).expand(B,1,H,W)\n        ys = torch.linspace(0,1,H,device=grids.device).view(1,1,H,1).expand(B,1,H,W)\n        x = torch.cat([x, xs, ys], dim=1)  # → (B, C+2, H, W)\n        # forward through convs\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        # global pooling → (B,hidden_dim)\n        return self.pool(x).view(B, -1)\n\n# 5) Siapkan DataLoader\nDATA_DIR   = '/kaggle/input/arc-prize-2025'\npath_train = os.path.join(DATA_DIR, 'arc-agi_training_challenges.json')\ntrain_ds   = ArcDataset(path_train, split='train')\ntrain_loader = DataLoader(train_ds,\n                          batch_size=16,\n                          shuffle=True,\n                          collate_fn=collate_fn,\n                          num_workers=0)\n\n# 6) Latih Siamese Encoder\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nencoder = PerceptionEncoderCoord(num_colors=11,\n                                 color_emb_dim=16,\n                                 hidden_dim=128).to(device)\noptimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n\ndef siamese_loss(e1, e2):\n    # ingin embedding input ≈ embedding output\n    cos = F.cosine_similarity(e1, e2, dim=-1)\n    return (1 - cos).mean()\n\nepochs = 10\nfor epoch in range(1, epochs+1):\n    encoder.train()\n    total_loss = 0.0\n    for inps, outs, _, _ in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n        inps, outs = inps.to(device), outs.to(device)\n        e1 = encoder(inps)\n        e2 = encoder(outs)\n        loss = siamese_loss(e1, e2)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * inps.size(0)\n    avg = total_loss / len(train_loader.dataset)\n    print(f\"[Epoch {epoch}] Avg Loss: {avg:.4f}\")\n\nprint(\"Perception Encoder training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:36:31.189546Z","iopub.execute_input":"2025-07-19T15:36:31.190134Z","iopub.status.idle":"2025-07-19T15:36:46.444672Z","shell.execute_reply.started":"2025-07-19T15:36:31.190112Z","shell.execute_reply":"2025-07-19T15:36:46.443782Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1/10:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0952bfb4eb364bf69dff6acacc269cef"}},"metadata":{}},{"name":"stdout","text":"[Epoch 1] Avg Loss: 0.0003\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/10:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f1b8a83b1954c9eb5605d5bc208c4cb"}},"metadata":{}},{"name":"stdout","text":"[Epoch 2] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/10:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"036f4f331d724b98af4229a80404b738"}},"metadata":{}},{"name":"stdout","text":"[Epoch 3] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/10:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57cc2f11d767406db19d0f4581a1faeb"}},"metadata":{}},{"name":"stdout","text":"[Epoch 4] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/10:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e2edcdc5db547d38e2a99beb9c6ab11"}},"metadata":{}},{"name":"stdout","text":"[Epoch 5] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/10:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f41bae1738f24daf8586e3d6ea060be0"}},"metadata":{}},{"name":"stdout","text":"[Epoch 6] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/10:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab430e7a17ca47978cc217d276b8c277"}},"metadata":{}},{"name":"stdout","text":"[Epoch 7] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/10:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21d62b140878468dbeb7ab77c96d81aa"}},"metadata":{}},{"name":"stdout","text":"[Epoch 8] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/10:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5af7a4d3efb44df884339122d10613c"}},"metadata":{}},{"name":"stdout","text":"[Epoch 9] Avg Loss: 0.0000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/10:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b17d574cb7da41df859967fe0ab6f3a5"}},"metadata":{}},{"name":"stdout","text":"[Epoch 10] Avg Loss: 0.0000\nPerception Encoder training complete.\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"# Desain DSL & Beam-Search","metadata":{}},{"cell_type":"code","source":"import os, json, numpy as np\nfrom tqdm.auto import tqdm\n\n# 1) Revisi ArcDataset untuk optional solutions_path\nclass ArcDataset(Dataset):\n    def __init__(self, challenges_path, solutions_path=None, split='train'):\n        raw = json.load(open(challenges_path))\n        # load tasks\n        if isinstance(raw, dict) and 'tasks' in raw:\n            tasks = raw['tasks']\n        elif isinstance(raw, dict) and all(isinstance(v, dict) for v in raw.values()):\n            tasks = []\n            for tid, rec in raw.items():\n                rec2 = rec.copy(); rec2['id'] = tid\n                tasks.append(rec2)\n        else:\n            tasks = raw\n\n        # ensure every task has 'id'\n        for rec in tasks:\n            if 'id' not in rec and 'task_id' in rec:\n                rec['id'] = rec['task_id']\n\n        self.tasks = tasks\n        self.split = split\n\n        # load solutions if provided\n        self.solutions = {}\n        if solutions_path and os.path.exists(solutions_path):\n            raw_s = json.load(open(solutions_path))\n            sol_list = raw_s.get('tasks') if isinstance(raw_s, dict) and 'tasks' in raw_s else (\n                       raw_s if isinstance(raw_s, list) else list(raw_s.values()))\n            for rec in sol_list:\n                if isinstance(rec, dict):\n                    tid = rec.get('id') or rec.get('task_id')\n                    self.solutions[tid] = rec\n\n    def __len__(self):\n        return len(self.tasks)\n\n    def __getitem__(self, idx):\n        rec = self.tasks[idx]\n        tid = rec['id']\n        train_pairs = [(np.array(ex['input']), np.array(ex['output'])) for ex in rec['train']]\n        test_grids  = [np.array(ex['input']) for ex in rec['test']]\n        sample = {'id': tid, 'train': train_pairs, 'test': test_grids}\n        if self.split == 'eval' and tid in self.solutions:\n            sol = self.solutions[tid]\n            if 'test' in sol:\n                sample['test_gt'] = [np.array(ex['output']) for ex in sol['test']]\n        return sample\n\n\n# 2) Evaluasi Beam‐Search Synthesizer\nDATA_DIR        = '/kaggle/input/arc-prize-2025'\neval_chal_path  = os.path.join(DATA_DIR, 'arc-agi_evaluation_challenges.json')\neval_sol_path   = os.path.join(DATA_DIR, 'arc-agi_evaluation_solutions.json')\n\n# sekarang ini akan cocok dengan signature baru\neval_ds = ArcDataset(eval_chal_path, solutions_path=eval_sol_path, split='eval')\n\nbeam_width, max_depth, alpha = 20, 5, 0.7\ncorrect, errors = 0, []\n\nfor sample in tqdm(eval_ds, desc='Synth eval'):\n    tid = sample['id']\n    try:\n        prog = synthesize_program(\n            sample['train'],\n            beam_width=beam_width,\n            max_depth=max_depth,\n            α=alpha\n        )\n        preds = [prog.apply(inp) for inp in sample['test']]\n        gts   = sample.get('test_gt', [])\n        if gts and all(\n            (p.shape == gt.shape and np.array_equal(p, gt))\n            for p,gt in zip(preds, gts)\n        ):\n            correct += 1\n    except Exception as e:\n        errors.append((tid, str(e)))\n\ntotal = len(eval_ds)\nprint(f'Beam-search accuracy: {correct}/{total} = {correct/total:.2%}')\nif errors:\n    print(f'Encountered errors on {len(errors)} tasks, e.g.:', errors[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:39:04.547424Z","iopub.execute_input":"2025-07-19T15:39:04.548195Z","iopub.status.idle":"2025-07-19T15:39:04.674273Z","shell.execute_reply.started":"2025-07-19T15:39:04.548165Z","shell.execute_reply":"2025-07-19T15:39:04.673412Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Synth eval:   0%|          | 0/120 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b520ce381dbc430a9f8751586e5917ba"}},"metadata":{}},{"name":"stdout","text":"Beam-search accuracy: 0/120 = 0.00%\nEncountered errors on 120 tasks, e.g.: [('0934a4d8', \"synthesize_program() got an unexpected keyword argument 'α'\"), ('135a2760', \"synthesize_program() got an unexpected keyword argument 'α'\"), ('136b0064', \"synthesize_program() got an unexpected keyword argument 'α'\"), ('13e47133', \"synthesize_program() got an unexpected keyword argument 'α'\"), ('142ca369', \"synthesize_program() got an unexpected keyword argument 'α'\")]\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"# Perception Encoder","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom collections import Counter\nfrom tqdm.auto import tqdm\n\n# ─── (1) ArcDataset ───────────────────────────────────────────────────────────\nclass ArcDataset(Dataset):\n    def __init__(self, chal_path, sol_path=None, split='train'):\n        raw = json.load(open(chal_path))\n        if isinstance(raw, dict) and 'tasks' in raw:\n            self.tasks = raw['tasks']\n        elif isinstance(raw, dict):\n            self.tasks = [{**rec, 'id':tid}\n                          for tid,rec in raw.items() if isinstance(rec, dict)]\n        else:\n            self.tasks = raw\n        self.solutions = {}\n        if sol_path and os.path.exists(sol_path):\n            raw_s = json.load(open(sol_path))\n            sol_list = (raw_s.get('tasks') if isinstance(raw_s, dict) and 'tasks' in raw_s\n                        else raw_s if isinstance(raw_s, list)\n                        else list(raw_s.values()))\n            for rec in sol_list:\n                if isinstance(rec, dict):\n                    tid = rec.get('id') or rec.get('task_id')\n                    self.solutions[tid] = rec\n        self.split = split\n\n    def __len__(self): return len(self.tasks)\n    def __getitem__(self, idx):\n        t = self.tasks[idx]\n        train_pairs = [(np.array(ex['input']), np.array(ex['output']))\n                       for ex in t['train']]\n        test_inputs = [np.array(ex['input']) for ex in t['test']]\n        tid = t.get('id') or t.get('task_id')\n        sample = {'train': train_pairs, 'test': test_inputs, 'id': tid}\n        if self.split=='eval' and tid in self.solutions:\n            sol = self.solutions[tid]\n            if 'test' in sol:\n                sample['test_gt'] = [np.array(ex['output']) for ex in sol['test']]\n        return sample\n\n# ─── (2) EncoderTrainDataset & pad_collate ───────────────────────────────────\nclass EncoderTrainDataset(Dataset):\n    def __init__(self, arc_ds):\n        self.pairs = [(inp, out) for s in arc_ds for inp,out in s['train']]\n    def __len__(self): return len(self.pairs)\n    def __getitem__(self, idx):\n        inp, out = self.pairs[idx]\n        return torch.from_numpy(inp).long(), torch.from_numpy(out).long()\n\ndef pad_collate(batch):\n    inps, outs = zip(*batch)\n    max_h = max(max(t.shape[0] for t in inps), max(t.shape[0] for t in outs))\n    max_w = max(max(t.shape[1] for t in inps), max(t.shape[1] for t in outs))\n    pad_in, pad_out = [], []\n    for i,o in zip(inps,outs):\n        hi,wi = i.shape; ho,wo = o.shape\n        pad_i = (0, max_w-wi, 0, max_h-hi)\n        pad_o = (0, max_w-wo, 0, max_h-ho)\n        pad_in.append(F.pad(i, pad_i, value=0))\n        pad_out.append(F.pad(o, pad_o, value=0))\n    return torch.stack(pad_in), torch.stack(pad_out)\n\n# ─── (3) PerceptionEncoderCNN ─────────────────────────────────────────────────\nclass PerceptionEncoderCNN(nn.Module):\n    def __init__(self, num_colors=11, emb_dim=128):\n        super().__init__()\n        self.embed = nn.Embedding(num_colors, 16)\n        self.conv1 = nn.Conv2d(16, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool  = nn.AdaptiveAvgPool2d((1,1))\n        self.fc    = nn.Linear(64, emb_dim)\n    def forward(self, g):\n        x = self.embed(g).permute(0,3,1,2)  # (B,16,H,W)\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = self.pool(x).view(g.size(0), -1)\n        return self.fc(x)\n\n# ─── (4) Setup device & checkpoint logic ─────────────────────────────────────\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nencoder = PerceptionEncoderCNN().to(device)\nckpt = 'perception_encoder_cnn.pth'\n\nif os.path.exists(ckpt):\n    encoder.load_state_dict(torch.load(ckpt, map_location=device))\n    print(\"✅ Loaded encoder checkpoint.\")\nelse:\n    print(\"⚠️  Checkpoint not found, training encoder from scratch for 5 epochs...\")\n    # prepare training loader\n    DATA_DIR = '/kaggle/input/arc-prize-2025'\n    arc_ds   = ArcDataset(\n        os.path.join(DATA_DIR,'arc-agi_training_challenges.json'),\n        os.path.join(DATA_DIR,'arc-agi_training_solutions.json'),\n        split='train'\n    )\n    train_loader = DataLoader(\n        EncoderTrainDataset(arc_ds),\n        batch_size=32,\n        shuffle=True,\n        collate_fn=pad_collate,\n        num_workers=0\n    )\n    opt = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n    for ep in range(1,6):\n        encoder.train()\n        total_loss = 0\n        for inp,out in tqdm(train_loader, desc=f\"Train Encoder Epoch {ep}/5\"):\n            inp, out = inp.to(device), out.to(device)\n            e1, e2 = encoder(inp), encoder(out)\n            loss   = (1 - F.cosine_similarity(e1, e2, dim=1)).mean()\n            opt.zero_grad(); loss.backward(); opt.step()\n            total_loss += loss.item() * inp.size(0)\n        avg = total_loss / len(train_loader.dataset)\n        print(f\"  Epoch {ep}, Avg Loss: {avg:.4f}\")\n    torch.save(encoder.state_dict(), ckpt)\n    print(\"✅ Encoder trained & checkpoint saved.\")\n\nencoder.eval()\n\n# Revisi fungsi embed_grid untuk mengatasi negative strides\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\n\ndef embed_grid(grid: np.ndarray) -> torch.Tensor:\n    # pastikan array kontigu dan tanpa stride negatif\n    arr = np.ascontiguousarray(grid.copy())\n    t   = torch.from_numpy(arr).long().unsqueeze(0).to(device)  # (1, H, W)\n    with torch.no_grad():\n        e = encoder(t)  # (1, emb_dim)\n    return F.normalize(e.squeeze(0), dim=0)  # (emb_dim,)\n\n# Ganti definisi embed_grid lama dengan yang di atas sebelum DSL & beam-search.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:39:13.639017Z","iopub.execute_input":"2025-07-19T15:39:13.639724Z","iopub.status.idle":"2025-07-19T15:39:13.665977Z","shell.execute_reply.started":"2025-07-19T15:39:13.639700Z","shell.execute_reply":"2025-07-19T15:39:13.665358Z"}},"outputs":[{"name":"stdout","text":"✅ Loaded encoder checkpoint.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"# Neuro-Symbolic Pipeline","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nfrom collections import Counter\nfrom tqdm.auto import tqdm\n\n# ─── (1) ArcDataset ───────────────────────────────────────────────────────────\nclass ArcDataset(Dataset):\n    def __init__(self, chal_path, sol_path=None, split='train'):\n        raw = json.load(open(chal_path))\n        if isinstance(raw, dict) and 'tasks' in raw:\n            self.tasks = raw['tasks']\n        elif isinstance(raw, dict):\n            self.tasks = [{**rec, 'id':tid} for tid,rec in raw.items() if isinstance(rec, dict)]\n        else:\n            self.tasks = raw\n        # load solutions if any\n        self.solutions = {}\n        if sol_path and os.path.exists(sol_path):\n            raw_s = json.load(open(sol_path))\n            sol_list = raw_s.get('tasks') if isinstance(raw_s, dict) and 'tasks' in raw_s else (\n                       raw_s if isinstance(raw_s, list) else list(raw_s.values()))\n            for rec in sol_list:\n                if isinstance(rec, dict):\n                    tid = rec.get('id') or rec.get('task_id')\n                    self.solutions[tid] = rec\n        self.split = split\n\n    def __len__(self): return len(self.tasks)\n    def __getitem__(self, idx):\n        t = self.tasks[idx]\n        # train pairs\n        train_pairs = [(np.array(ex['input']), np.array(ex['output'])) for ex in t['train']]\n        # test inputs\n        test_inputs = [np.array(ex['input']) for ex in t['test']]\n        tid = t.get('id') or t.get('task_id')\n        sample = {'train': train_pairs, 'test': test_inputs, 'id': tid}\n        # optionally test ground-truth\n        if self.split=='eval' and tid in self.solutions:\n            sol = self.solutions[tid]\n            if 'test' in sol:\n                sample['test_gt'] = [np.array(ex['output']) for ex in sol['test']]\n        return sample\n\n# ─── (2) PerceptionEncoderCNN (pretrained) ───────────────────────────────────\nclass PerceptionEncoderCNN(nn.Module):\n    def __init__(self, num_colors=11, emb_dim=128):\n        super().__init__()\n        self.embed = nn.Embedding(num_colors, 16)\n        self.conv1 = nn.Conv2d(16, 32, 3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool  = nn.AdaptiveAvgPool2d((1,1))\n        self.fc    = nn.Linear(64, emb_dim)\n    def forward(self, g):\n        # g: (B,H,W)\n        x = self.embed(g).permute(0,3,1,2)  # (B,16,H,W)\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = self.pool(x).view(g.size(0), -1)\n        return self.fc(x)\n\n# ─── (3) Load encoder checkpoint & embed helper (revisi) ─────────────────────\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nencoder = PerceptionEncoderCNN().to(device)\nencoder.load_state_dict(torch.load('perception_encoder_cnn.pth', map_location=device))\nencoder.eval()\n\ndef embed_grid(grid: np.ndarray) -> torch.Tensor:\n    # Salin dan buat kontigu agar tidak ada negative-stride\n    arr = np.ascontiguousarray(grid.copy())\n    t   = torch.from_numpy(arr).long().unsqueeze(0).to(device)  # (1, H, W)\n    with torch.no_grad():\n        e = encoder(t)  # (1, emb_dim)\n    return F.normalize(e.squeeze(0), dim=0)  # (emb_dim,)\n\n# ─── (4) DSL primitives & scoring ────────────────────────────────────────────\ndef identity(x): return x.copy()\ndef rot90(x):    return np.rot90(x,1)\ndef rot180(x):   return np.rot90(x,2)\ndef rot270(x):   return np.rot90(x,3)\ndef flip_h(x):   return np.fliplr(x)\ndef flip_v(x):   return np.flipud(x)\n\nBASE_OPS = {\n    'identity': identity, 'rot90': rot90,\n    'rot180': rot180,       'rot270': rot270,\n    'flip_h': flip_h,       'flip_v': flip_v\n}\n\nclass Program:\n    def __init__(self, ops=None): self.ops = ops or []\n    def apply(self, g):\n        out = g.copy()\n        for _,fn in self.ops: out = fn(out)\n        return out\n    def extend(self, name, fn): return Program(self.ops + [(name, fn)])\n    def __repr__(self): return ' -> '.join(n for n,_ in self.ops) or 'identity'\n\ndef score_program(prog, pairs, α=0.7):\n    pix_corr = 0; pix_tot = 0; sim_sum = 0.0\n    for inp,tgt in pairs:\n        pred = prog.apply(inp)\n        if pred.shape == tgt.shape:\n            pix_corr += np.sum(pred == tgt)\n        pix_tot += tgt.size\n        sim_sum += float(torch.dot(embed_grid(pred), embed_grid(tgt)))\n    pix_score = pix_corr / pix_tot\n    sim_score = sim_sum / len(pairs)\n    return α * pix_score + (1 - α) * sim_score\n\n# ─── (5) Beam-search synthesizer ──────────────────────────────────────────────\ndef synthesize_program(pairs, beam_width=5, max_depth=3, α=0.7):\n    # dynamic map_colors\n    cnt = Counter()\n    for inp,out in pairs:\n        if inp.shape!=out.shape: continue\n        mask = inp != out\n        for a,b in zip(inp[mask].flatten(), out[mask].flatten()):\n            cnt[(a,b)] += 1\n    mapping = {}\n    by_old = {}\n    for (o,n),c in cnt.items():\n        if o not in by_old or c>by_old[o][1]:\n            by_old[o] = (n,c)\n    for o,(n,_) in by_old.items():\n        mapping[o] = n\n\n    def map_colors(x, m=mapping):\n        y = x.copy()\n        for o,n in m.items(): y[x==o] = n\n        return y\n\n    OPS = dict(BASE_OPS)\n    if mapping: OPS['map_colors'] = map_colors\n\n    beam = [Program()]; best = beam[0]; best_sc = score_program(best, pairs, α)\n    for _ in range(max_depth):\n        candidates = []\n        for prog in beam:\n            for name,fn in OPS.items():\n                p2 = prog.extend(name, fn)\n                sc = score_program(p2, pairs, α)\n                candidates.append((sc, p2))\n                if sc > best_sc:\n                    best_sc, best = sc, p2\n        candidates.sort(key=lambda x: x[0], reverse=True)\n        beam = [p for _,p in candidates[:beam_width]]\n    return best\n\n# ─── (6) NeuroSymbolicSolver & final eval ────────────────────────────────────\nclass NeuroSymbolicSolver:\n    def __init__(self, beam_width=5, max_depth=3, α=0.7):\n        self.bw, self.md, self.α = beam_width, max_depth, α\n    def solve(self, train_pairs, test_inputs):\n        prog = synthesize_program(train_pairs, self.bw, self.md, self.α)\n        return [prog.apply(inp) for inp in test_inputs], prog\n\n# evaluate on public‐eval\nDATA_DIR = '/kaggle/input/arc-prize-2025'\neval_ds = ArcDataset(\n    os.path.join(DATA_DIR,'arc-agi_evaluation_challenges.json'),\n    sol_path=os.path.join(DATA_DIR,'arc-agi_evaluation_solutions.json'),\n    split='eval'\n)\nsolver = NeuroSymbolicSolver(beam_width=5, max_depth=3, α=0.7)\ncorrect = 0\nfor sample in tqdm(eval_ds, desc='Eval'):\n    preds,_ = solver.solve(sample['train'], sample['test'])\n    gts = sample.get('test_gt', [])\n    if gts and all(np.array_equal(p,gt) for p,gt in zip(preds,gts)):\n        correct += 1\n\nprint(f'Final Neuro-Symbolic accuracy: {correct}/{len(eval_ds)} = {correct/len(eval_ds):.2%}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:39:18.502478Z","iopub.execute_input":"2025-07-19T15:39:18.502746Z","iopub.status.idle":"2025-07-19T15:39:50.241105Z","shell.execute_reply.started":"2025-07-19T15:39:18.502727Z","shell.execute_reply":"2025-07-19T15:39:50.240335Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Eval:   0%|          | 0/120 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f138f6fe6ea4c5f8aa21295efc4bd78"}},"metadata":{}},{"name":"stdout","text":"Final Neuro-Symbolic accuracy: 0/120 = 0.00%\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"# Training & Validasi","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport torch\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom tqdm.auto import tqdm\n\n# ─── (0) Assumptions ──────────────────────────────────────────────────────────\n# You have already defined or imported:\n# - ArcDataset\n# - NeuroSymbolicSolver\n# Replace the import below if you have these in a module, e.g.:\n# from arc_pipeline import ArcDataset, NeuroSymbolicSolver\n\n# ─── (1) Setup ───────────────────────────────────────────────────────────────\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\nDATA_DIR    = '/kaggle/input/arc-prize-2025'\nTRAIN_CHAL  = os.path.join(DATA_DIR, 'arc-agi_training_challenges.json')\nTRAIN_SOL   = os.path.join(DATA_DIR, 'arc-agi_training_solutions.json')\n\n# Load the full training dataset\nfull_ds = ArcDataset(TRAIN_CHAL, TRAIN_SOL, split='train')\nsamples = [full_ds[i] for i in range(len(full_ds))]\nprint(f\"Total tasks: {len(samples)}\")\n\n# ─── (2) Split 80/20 ─────────────────────────────────────────────────────────\ntrain_samples, val_samples = train_test_split(\n    samples, test_size=0.2, random_state=42\n)\nprint(f\"Train tasks: {len(train_samples)}, Val tasks: {len(val_samples)}\")\n\n# ─── (3) Evaluation Helper ───────────────────────────────────────────────────\ndef evaluate_solver(solver, samples):\n    correct = 0\n    for sample in samples:\n        preds, _ = solver.solve(sample['train'], sample['test'])\n        gts = sample.get('test_gt', [])\n        if gts and all(np.array_equal(p, gt) for p, gt in zip(preds, gts)):\n            correct += 1\n    return correct, len(samples)\n\n# ─── (4) Hyperparameter Grid Search ──────────────────────────────────────────\nparam_grid = {\n    'beam_width': [3, 5, 7],\n    'max_depth' : [2, 3, 4],\n    'alpha'     : [0.5, 0.7, 0.9]\n}\n\nrecords = []\nfor params in tqdm(list(ParameterGrid(param_grid)), desc='Grid Search'):\n    solver = NeuroSymbolicSolver(\n        beam_width=params['beam_width'],\n        max_depth=params['max_depth'],\n        α=params['alpha']\n    )\n    train_corr, train_tot = evaluate_solver(solver, train_samples)\n    val_corr,   val_tot   = evaluate_solver(solver, val_samples)\n    records.append({\n        **params,\n        'train_acc': train_corr / train_tot,\n        'val_acc'  : val_corr   / val_tot\n    })\n\n# ─── (5) Results ─────────────────────────────────────────────────────────────\ndf = pd.DataFrame(records).sort_values('val_acc', ascending=False).reset_index(drop=True)\nprint(\"\\nTop configurations by validation accuracy:\")\nprint(df.head(10).to_string(index=False))\n\nbest = df.iloc[0]\nprint(f\"\\nBest config: beam_width={best.beam_width}, max_depth={best.max_depth}, alpha={best.alpha}\")\nprint(f\"Train Acc: {best.train_acc:.2%}, Val Acc: {best.val_acc:.2%}\")\n\n# Save results for further analysis\ndf.to_csv('grid_search_results.csv', index=False)\nprint(\"\\nGrid search results saved to grid_search_results.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:39:56.225113Z","iopub.execute_input":"2025-07-19T15:39:56.225440Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nTotal tasks: 1000\nTrain tasks: 800, Val tasks: 200\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Grid Search:   0%|          | 0/27 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db5fbc0e0dfb44b4bc87a46dcc67e630"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# Profiling Biaya & Kecepatan","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport time\nimport pandas as pd\nimport torch\nfrom tqdm.auto import tqdm\n\n# ─── PASTIKAN sebelumnya sudah terdefinisi di notebook:\n# ArcDataset, PerceptionEncoderCNN, NeuroSymbolicSolver, BASE_OPS, device\n\n# ─── (1) Muat evaluation set ─────────────────────────────────────────────────\nDATA_DIR = '/kaggle/input/arc-prize-2025'\neval_ds = ArcDataset(\n    os.path.join(DATA_DIR, 'arc-agi_evaluation_challenges.json'),\n    sol_path=os.path.join(DATA_DIR, 'arc-agi_evaluation_solutions.json'),\n    split='eval'\n)\n\n# ─── (2) Hyperparams terbaik ──────────────────────────────────────────────────\nbest_beam, best_depth, alpha = 5, 3, 0.7\n\n# ─── (3) Load encoder & instantiate solver ───────────────────────────────────\nencoder = PerceptionEncoderCNN(num_colors=11, emb_dim=128).to(device)\nencoder.load_state_dict(torch.load('perception_encoder_cnn.pth', map_location=device))\nencoder.eval()\n\n# **Perhatikan**: parameter nama adalah `α`, sesuai definisi __init__\nsolver = NeuroSymbolicSolver(\n    beam_width=best_beam,\n    max_depth=best_depth,\n    α=alpha\n)\n\n# ─── (4) Profiling tiap tugas ─────────────────────────────────────────────────\nrecords = []\nfor sample in tqdm(eval_ds, desc=\"Profiling tasks\"):\n    tid         = sample['id']\n    train_pairs = sample['train']\n    test_inputs = sample['test']\n\n    # Warm-up supaya JIT/ops sudah ter-load\n    _ = solver.solve(train_pairs, test_inputs)\n\n    # Ukur waktu solve()\n    t0 = time.perf_counter()\n    preds, prog = solver.solve(train_pairs, test_inputs)\n    dt = time.perf_counter() - t0\n\n    # Estimasi kandidat per depth: base_ops + fill_ops + shift_ops\n    num_base  = len(BASE_OPS)\n    num_fill  = len(set().union(*(np.unique(inp) for inp,_ in train_pairs)))\n    num_shift = 4  # dx/dy ∈ {(-1,1)} => 4 kombinasi\n    est_cands = best_beam * best_depth * (num_base + num_fill + num_shift)\n\n    records.append({\n        'task_id'       : tid,\n        'time_solve_s'  : dt,\n        'est_candidates': est_cands,\n        'prog_len'      : len(prog.ops)\n    })\n\n# ─── (5) Ringkasan dengan pandas ───────────────────────────────────────────────\ndf = pd.DataFrame(records)\nprint(f\"Total tasks profiled: {len(df)}\")\nprint(df['time_solve_s'].describe().rename(index={\n    'mean':'Avg time/task',\n    '50%' :'Median time/task',\n    'max' :'Max time/task',\n    'min' :'Min time/task'\n}))\nprint(\"\\nTop 5 slowest tasks:\")\nprint(df.nlargest(5, 'time_solve_s')[['task_id','time_solve_s','prog_len','est_candidates']])\n\n# ─── (6) Simpan hasil ─────────────────────────────────────────────────────────\ndf.to_csv('profiling_results.csv', index=False)\nprint(\"\\nProfiling results saved to profiling_results.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:31:27.544197Z","iopub.status.idle":"2025-07-19T15:31:27.544553Z","shell.execute_reply.started":"2025-07-19T15:31:27.544342Z","shell.execute_reply":"2025-07-19T15:31:27.544360Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tuning & Iterasi","metadata":{}},{"cell_type":"code","source":"# ===============================\n# 6) TUNING & ITERASI (Fixed)\n# ===============================\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom tqdm.auto import tqdm\nfrom numpy import ascontiguousarray\n\n# ——————————————————————————————————————————————————————————————————————\n# Prasyarat: Pastikan di cell lain sudah terdefinisi:\n#   - ArcDataset\n#   - PerceptionEncoderCNN\n#   - synthesize_program  (yang sudah include map_colors)\n#   - NeuroSymbolicSolver\n# ——————————————————————————————————————————————————————————————————————\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Using device:\", device)\n\n# 1) Muat & split data\nDATA_DIR     = '/kaggle/input/arc-prize-2025'\nTRAIN_CHAL   = os.path.join(DATA_DIR, 'arc-agi_training_challenges.json')\nTRAIN_SOL    = os.path.join(DATA_DIR, 'arc-agi_training_solutions.json')\nfull_ds      = ArcDataset(TRAIN_CHAL, TRAIN_SOL, split='train')\nall_samples  = [full_ds[i] for i in range(len(full_ds))]\ntrain_samples, val_samples = train_test_split(\n    all_samples, test_size=0.2, random_state=42\n)\nprint(f\"Train tasks: {len(train_samples)}, Val tasks: {len(val_samples)}\")\n\n# 2) Load pretrained encoder (CNN) sekali saja\nencoder_base = PerceptionEncoderCNN(num_colors=11, emb_dim=128).to(device)\nckpt = 'perception_encoder_cnn.pth'\nif not os.path.exists(ckpt):\n    raise FileNotFoundError(f\"Checkpoint '{ckpt}' tidak ditemukan – jalankan pelatihan encoder terlebih dahulu\")\nencoder_base.load_state_dict(torch.load(ckpt, map_location=device))\nencoder_base.eval()\n\n# 3) embed_grid: pastikan array kontigu tanpa negative strides\ndef embed_grid(grid: np.ndarray) -> torch.Tensor:\n    arr = ascontiguousarray(grid.copy())\n    t   = torch.from_numpy(arr).long().unsqueeze(0).to(device)  # (1, H, W)\n    with torch.no_grad():\n        e = encoder_base(t)  # (1, emb_dim)\n    return F.normalize(e.squeeze(0), dim=0)  # (emb_dim,)\n\n# 4) Helper evaluasi solver\ndef evaluate_solver(solver, samples):\n    correct = 0\n    for sample in samples:\n        preds, _ = solver.solve(sample['train'], sample['test'])\n        gts = sample.get('test_gt', [])\n        if gts and all(p.shape==gt.shape and np.array_equal(p,gt)\n                       for p,gt in zip(preds, gts)):\n            correct += 1\n    return correct, len(samples)\n\n# 5) Grid search hyperparameter solver-only\nparam_grid = {\n    'beam_width': [5, 10, 20],\n    'max_depth' : [3, 5],\n    'alpha'     : [0.5, 0.7, 0.9]\n}\n\nrecords = []\nfor params in tqdm(ParameterGrid(param_grid), desc='Tuning & Iterasi'):\n    solver = NeuroSymbolicSolver(\n        beam_width=params['beam_width'],\n        max_depth=params['max_depth'],\n        alpha=params['alpha']    # konsisten nama    \n    )\n    trc, trt = evaluate_solver(solver, train_samples)\n    vcc, vct = evaluate_solver(solver, val_samples)\n    records.append({\n        **params,\n        'train_acc': trc / trt,\n        'val_acc'  : vcc / vct\n    })\n    print(f\" → bw={params['beam_width']}, md={params['max_depth']}, α={params['alpha']} \"\n          f\"=> train={trc/trt:.2%}, val={vcc/vct:.2%}\")\n\n# 6) Ringkasan & pilih konfigurasi terbaik\ndf = pd.DataFrame(records).sort_values('val_acc', ascending=False).reset_index(drop=True)\nprint(\"\\nTop 5 konfigurasi menurut val_acc:\")\nprint(df.head(5).to_string(index=False))\n\nbest = df.iloc[0]\nprint(f\"\\nBest config: beam_width={best.beam_width}, max_depth={best.max_depth}, alpha={best.alpha}\")\nprint(f\"Val Acc: {best.val_acc:.2%}\")\n\n# 7) Simpan hasil tuning\ndf.to_csv('tuning_iterations.csv', index=False)\nprint(\"Hasil tuning disimpan ke tuning_iterations.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:31:27.545413Z","iopub.status.idle":"2025-07-19T15:31:27.545725Z","shell.execute_reply.started":"2025-07-19T15:31:27.545564Z","shell.execute_reply":"2025-07-19T15:31:27.545580Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluasi Akhir & Submission","metadata":{}},{"cell_type":"code","source":"# ─── 7) Final Evaluation & Submission (Fixed) ────────────────────────────────\n\nimport os\nimport json\nimport numpy as np\nimport torch\nfrom tqdm.auto import tqdm\n\n# — 0) Device & hyperparameters (ganti sesuai hasil tuning!) —\ndevice     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbest_beam  = 5      # ← hasil tuning Anda\nbest_depth = 3      # ← hasil tuning Anda\nbest_alpha = 0.7    # ← hasil tuning Anda\n\n# — 1) Paths —\nDATA_DIR       = '/kaggle/input/arc-prize-2025'\neval_chal_path = os.path.join(DATA_DIR, 'arc-agi_evaluation_challenges.json')\neval_sol_path  = os.path.join(DATA_DIR, 'arc-agi_evaluation_solutions.json')\ntest_chal_path = os.path.join(DATA_DIR, 'arc-agi_test_challenges.json')\n\n# — 2) Load evaluation set (with GT) —\neval_ds = ArcDataset(\n    eval_chal_path,\n    sol_path=eval_sol_path,\n    split='eval'\n)\n\n# — 3) Load encoder checkpoint & build embed helper —\nencoder = PerceptionEncoderCNN(num_colors=11, emb_dim=128).to(device)\nencoder.load_state_dict(torch.load('perception_encoder_cnn.pth', map_location=device))\nencoder.eval()\n\ndef embed_grid(grid: np.ndarray) -> torch.Tensor:\n    arr = np.ascontiguousarray(grid.copy())\n    t   = torch.from_numpy(arr).long().unsqueeze(0).to(device)\n    with torch.no_grad():\n        e = encoder(t)\n    return torch.nn.functional.normalize(e.squeeze(0), dim=0)\n\n# — 4) Instantiate solver —  ✏️ **revisi di sini**\n#    gunakan posisi: (beam_width, max_depth, alpha)\nsolver = NeuroSymbolicSolver(best_beam, best_depth, best_alpha)\n\n# — 5) Final evaluation on public-eval —\ncorrect = 0\nfor sample in tqdm(eval_ds, desc='Final Eval'):\n    preds, _ = solver.solve(sample['train'], sample['test'])\n    gts      = sample.get('test_gt', [])\n    if gts and all(p.shape==gt.shape and np.array_equal(p,gt)\n                   for p,gt in zip(preds, gts)):\n        correct += 1\n\ntotal = len(eval_ds)\nprint(f'Final Eval Accuracy: {correct}/{total} = {correct/total:.2%}')\n\n# — 6) Generate submission on test set —\ntest_ds = ArcDataset(test_chal_path, sol_path=None, split='test')\n\nsubmission = {}\nfor sample in tqdm(test_ds, desc='Building Submission'):\n    tid    = sample['id']\n    preds,_ = solver.solve(sample['train'], sample['test'])\n    entries = []\n    for inp, pred in zip(sample['test'], preds):\n        if pred.shape != inp.shape:\n            pred = np.zeros_like(inp)\n        entries.append({\n            'attempt_1': pred.astype(int).tolist(),\n            'attempt_2': pred.astype(int).tolist()\n        })\n    submission[tid] = entries\n\n# — 7) Sanity checks & write out —\ntask_ids = {t['id'] for t in test_ds.tasks}\nassert set(submission.keys()) == task_ids, \"Ada task_id yang hilang!\"\nfor rec in test_ds.tasks:\n    tid      = rec['id']\n    expected = len(rec['test'])\n    actual   = len(submission[tid])\n    assert actual == expected, f\"{tid}: expecting {expected}, got {actual}\"\n\nwith open('submission.json','w') as f:\n    json.dump(submission, f)\nprint(f\"submission.json written with {len(submission)} tasks.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:31:27.547035Z","iopub.status.idle":"2025-07-19T15:31:27.547327Z","shell.execute_reply.started":"2025-07-19T15:31:27.547192Z","shell.execute_reply":"2025-07-19T15:31:27.547208Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─── 7) Final Evaluation & Submission (Sederhana) ─────────────────────────────\n\nimport os\nimport json\nimport numpy as np\nimport torch\nfrom tqdm.auto import tqdm\n\n# Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 1) Instantiate solver & load pretrained encoder\nbest_beam, best_depth, best_alpha = 20, 5, 0.7  # sesuaikan hasil tuning\ncolor_emb, hidden_dim = 16, 128\n\nencoder = PerceptionEncoder(\n    num_colors=11,\n    color_emb_dim=color_emb,\n    hidden_dim=hidden_dim\n).to(device)\nencoder.load_state_dict(torch.load('perception_encoder_trained.pth', map_location=device))\nencoder.eval()\n\nsolver = NeuroSymbolicSolver(\n    encoder=encoder,\n    beam_width=best_beam,\n    max_depth=best_depth,\n    alpha=best_alpha,\n    device=device\n)\n\n# 2) Load test set\nDATA_DIR  = '/kaggle/input/arc-prize-2025'\ntest_path = os.path.join(DATA_DIR, 'arc-agi_test_challenges.json')\ntest_ds   = ArcDataset(test_path, solutions_path=None, split='test')\n\n# 3) Bangun submission\nsubmission = {}\nfor sample in tqdm(test_ds, desc='Building Submission'):\n    tid         = sample['id']\n    train_pairs = sample['train']\n    test_inputs = sample['test']\n    preds, _    = solver.solve(train_pairs, test_inputs)\n\n    entries = []\n    for inp, pred in zip(test_inputs, preds):\n        # fallback jika ukuran mismatch\n        if pred.shape != inp.shape:\n            pred = np.zeros_like(inp)\n        arr = pred.astype(int).tolist()\n        entries.append({'attempt_1': arr, 'attempt_2': arr})\n\n    submission[tid] = entries\n\n# 4) Sanity checks\ntask_ids = {t['id'] for t in test_ds.challenges}\nassert set(submission.keys()) == task_ids, \"Ada task yang hilang!\"\nfor t in test_ds.challenges:\n    tid = t['id']\n    assert len(submission[tid]) == len(t['test']), (\n        f\"{tid}: jumlah output {len(submission[tid])} != expected {len(t['test'])}\"\n    )\n\n# 5) Tulis file\nwith open('submission.json','w') as f:\n    json.dump(submission, f)\nprint(f\"submission.json ditulis dengan {len(submission)} tasks.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:31:27.548326Z","iopub.status.idle":"2025-07-19T15:31:27.548634Z","shell.execute_reply.started":"2025-07-19T15:31:27.548497Z","shell.execute_reply":"2025-07-19T15:31:27.548512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ─── Validate submission.json ────────────────────────────────────────────────\n\nimport json\nimport os\nfrom torch.utils.data import DataLoader\n\n# 1) Pastikan file ada\nif not os.path.exists('submission.json'):\n    raise FileNotFoundError(\"submission.json not found in current directory.\")\nprint(\"Found submission.json\")\n\n# 2) Muat submission\nwith open('submission.json') as f:\n    submission = json.load(f)\nassert isinstance(submission, dict), \"submission.json harus berisi dict\"\n\n# 3) Load test challenges to check counts\nDATA_DIR   = '/kaggle/input/arc-prize-2025'\ntest_path  = os.path.join(DATA_DIR, 'arc-agi_test_challenges.json')\ntest_ds    = ArcDataset(test_path, solutions_path=None, split='test')\n\n# 4) Per-task checks\nfor rec in test_ds.challenges:\n    tid      = rec['id']\n    assert tid in submission, f\"Task ID {tid} missing in submission\"\n    outputs  = submission[tid]\n    # Shape check: must be list of dicts with 2 keys\n    assert isinstance(outputs, list), f\"Outputs for {tid} must be a list\"\n    expected = len(rec['test'])\n    actual   = len(outputs)\n    assert actual == expected, f\"Task {tid}: expected {expected} entries, got {actual}\"\n    for entry in outputs:\n        assert set(entry.keys()) == {'attempt_1','attempt_2'}, (\n            f\"Each entry for {tid} must have exactly 'attempt_1' and 'attempt_2' keys\"\n        )\n        # Ensure the arrays have correct shape\n        # We can check list lengths match input shape\n        inp_shape = np.array(rec['test'][0]).shape\n        for key in ('attempt_1','attempt_2'):\n            arr = np.array(entry[key])\n            assert arr.shape == inp_shape, (\n                f\"{key} for task {tid} has shape {arr.shape}, expected {inp_shape}\"\n            )\n\nprint(\"\\nsubmission.json format and contents look good!\\n\")\n\n# 5) Display first 5 entries for manual inspection\nprint(\"=== First 5 submission entries ===\")\nfor idx, tid in enumerate(list(submission.keys())[:5]):\n    outputs = submission[tid]\n    print(f\"\\nTask ID: {tid} (n_outputs={len(outputs)})\")\n    for j, entry in enumerate(outputs):\n        print(f\"  Output #{j+1}:\")\n        print(\"    attempt_1:\", entry['attempt_1'])\n        print(\"    attempt_2:\", entry['attempt_2'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T15:31:27.549703Z","iopub.status.idle":"2025-07-19T15:31:27.550072Z","shell.execute_reply.started":"2025-07-19T15:31:27.549892Z","shell.execute_reply":"2025-07-19T15:31:27.549907Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Dokumentasi & Paper Draft**","metadata":{}}]}